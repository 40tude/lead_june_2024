{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://uploads-ssl.webflow.com/62233c592d2a1e009d42f46c/6414802c0a2bea367cbc795b_logo-jedha-square.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: left; color:#20a08d; font-size: 40px\"><span><strong>Q Learning dans l'environnement Frozen Lake 4 x 4\n",
    "</strong></span></h1>\n",
    "\n",
    "Cet exercice vous mettra au défi de résoudre le problème d'apprentissage par renforcement dans l'environnement **Frozen Lake** de Gym, cela semble assez simple mais attention ! Il peut réserver quelques surprises !\n",
    "\n",
    "Le **Frozen Lake** consiste à traverser un lac gelé du départ (S)tart au but (G)oal sans tomber dans aucun trou (H)ole en marchant sur le lac gelé (F)rozen.\n",
    "\n",
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>Espace d'actions\n",
    "</strong></span></h2>\n",
    "\n",
    "L'agent utilise un vecteur à 1 élément pour les actions. L'espace d'action est (dir), où dir décide de la direction dans laquelle se déplacer, ce qui peut être :\n",
    "\n",
    "- 0 : GAUCHE\n",
    "\n",
    "- 1 : BAS\n",
    "\n",
    "- 2 : À DROITE\n",
    "\n",
    "- 3 : HAUT\n",
    "\n",
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>Espace d'états\n",
    "</strong></span></h2>\n",
    "\n",
    "L'état est une valeur représentant la position actuelle de l'agent sous la forme <code>current_row * nrows + current_col</code> (où la ligne et la colonne commencent à 0). Par exemple, la position de l'objectif sur la carte 4x4 peut être calculée comme suit : 3 * 4 + 3 = 15. \n",
    "\n",
    "Le nombre d'états possibles dépend de la taille de la carte. Par exemple, la carte 4x4 a 16 états possibles.\n",
    "\n",
    "\n",
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>Récompenses\n",
    "</strong></span></h2>\n",
    "\n",
    "- Récompenses prévues dans l'environnement:\n",
    "\n",
    "- Atteindre l'objectif (G)oal: +1\n",
    "\n",
    "- Tomber dans un trou (H)ole: 0 (Termine l'épisode)\n",
    "\n",
    "- Marcher sur les cases gélées du lac (F)rozen: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>1. Commençons par installer quelques bibliothèques\n",
    "</strong></span></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install cmake \n",
    "#!pip3 install scipy \n",
    "#!pip3 install gym[toy_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>2. Utilisez gym pour créer un environnement de <strong>Frozen Lake</strong> avec une dimension 4x4 qui n'est pas glissant. </strong></span></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez apprendre comment faire cela [ici](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/) . Réinitialisez l'environnement et affichez le premier état à l'aide de matplotlib et de la méthode `.render()` et  `render_mode=\"rgb_array\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>3. Réinitialisez l'environnement et regardez ce qui se passe lorsque vous effectuez l'action 0\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>4. Comment interprétez-vous les valeurs obtenues ?\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>5. Affichez la taille de l'espace d'action et de l'espace d'observation à l'aide des attributs de l'objet environnement.\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>6. Configurez la table Q (rappelez-vous qu'elle doit représenter les valeurs de chaque paire d'action-état), initialisez toutes les valeurs à zéro.\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Run a Q-Learning Loop inspired from the demo over 10 000 episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>7. Exécutez une boucle Q-Learning inspirée de la démo sur 10 000 épisodes.\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>8. Essayez de visualiser ce que fait l'agent en utilisant .render. Est-ce que quelque chose vous surprend ? Pourquoi pensez-vous que cela arrive ?\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. The agent does not move! Although we know for a fact this is not the optimal behaviour! The problem comes from the fact that we start with a Q-table filled with zeros. That means that at the beginning, the optimal action given by `np.argmax(q_table[state])` is always `0` which lead our agent against the wall. The odds of picking random actions are really low with our chosen policy (e.g. 0.1). In this setting, it becomes almost impossible for the agent to randomly reach the goal (the only non zero reward) and start learning!\n",
    "\n",
    "Try to think of a solution for this for at least 5 minutes then click the spoiler to get a clue:\n",
    "\n",
    "<details>\n",
    "<summary>SPOILER</summary>\n",
    "The solution is to force the agent to pick a random action whenever the score for all actions are equivalent\n",
    "</details>\n",
    "\n",
    "Once you think you have found a way to solve this issue, rerun your adapted training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'agent ne bouge pas ! Bien que nous sachions pertinemment que ce n'est pas le comportement optimal ! Le problème vient du fait que nous commençons avec une Q-table remplie de zéros. Cela signifie qu'au départ, l'action optimale donnée par `np.argmax(q_table[state])` est toujours `0` celle qui conduit notre agent contre le mur. \n",
    "\n",
    "Les chances de choisir des actions aléatoires sont vraiment faibles avec notre politique choisie (par exemple 0,1). Dans ce cas, il devient quasiment impossible pour l'agent d'atteindre aléatoirement l'objectif (la seule récompense non nulle) et de commencer à apprendre !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>9. Essayez de penser à une solution pour cela pendant au moins 5 minutes, puis cliquez sur le spoiler pour obtenir un indice. Une fois que vous pensez avoir trouvé un moyen de résoudre ce problème, relancez votre boucle d'entraînement avec les modifications adaptées.\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>SPOILER</summary>\n",
    "La solution est de forcer l'agent à choisir une action au hasard chaque fois que la valeur de toutes les actions est équivalent.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>10. Calculez la récompense moyenne sur 100 épisodes\n",
    ".\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>11. Que fait l'agent maintenant ? Montrez visuellement son comportement.\n",
    ".\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! L'agent est désormais en mesure de gagner la partie de manière optimale !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: left; color:#20a08d; font-size: 30px\"><span><strong>Q Learning dans l'environnement Frozen Lake 8 x 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, voyons si nous pouvons résoudre le problème du Frozen Lake avec une carte plus difficile !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>1. Configurez un environnement Frozen Lake avec une carte 8x8 et non glissante\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cadre, la probabilité d'atteindre l'objectif au hasard est beaucoup plus mince, voyons si notre boucle d'entraînement a une chance de compléter l'algorithme Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>2. Configurez la table Q avec des valeurs initiales égales à zéro.\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>3. Exécutez une boucle Q-Learning inspirée de la démo sur 10 000 épisodes.\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>4. Visualisez le comportement de l'agent.\n",
    "\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On dirait qu'il a atteint le but !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: left; color:#20a08d; font-size: 30px\"><span><strong>Q Learning dans l'environnement Frozen Lake 8 x 8 glissant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, compliquons encore plus les choses en rendant le lac glissant ! \n",
    "\n",
    "Cela signifie que chaque fois que vous choisissez une action, vous avez deux chances sur trois d'aller latéralement au lieu d'avancer (également avec un tiers de chance). Par exemple, si je choisis l'action \"en bas\", la probabilité d'aller \"en bas\" est $\\frac{1}{3}$, la probabilité d'aller \"à gauche\" est $\\frac{1}{3}$ et la probabilité d'aller \"à droite\" est $\\frac{1}{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>1. Configurez un environnement avec une carte 8x8 en mode glissant.\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>2. Configurer la table Q\n",
    "\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>3. Exécutez l'algorithme d'apprentissage Q sur 100 000 épisodes</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>4. Visualisez ce que fait l'agent\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>5. On dirait que l'agent bouge, voyons quelle est sa récompense moyenne sur cent épisodes dans le cadre de la politique gloutonne.\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>6. L'agent n'a rien appris, puisqu'il a atteint l'objectif dans 6% des essais. Existe-t-il un moyen d'obtenir une récompense de 100 % du temps ? Essayez d'exécuter l'algorithme sur 500 000 étapes supplémentaires pour voir si nous améliorons notre score !\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>7. On dirait que dans ce cas, plus d'entraînement ne nous permet pas de gagner à chaque fois ! Essayez de choisir les actions manuellement pour avoir 100 % de chances de gagner !\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est techniquement possible d'atteindre l'objectif avec une probabilité de 100 % en choisissant la bonne politique exacte. Et même si l'algorithme Q-Learning devrait finalement converger vers la politique optimale, il peut être très coûteux en calcul d'y parvenir, même avec un problème aussi simple !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16+"
  },
  "vscode": {
   "interpreter": {
    "hash": "fbc4d3870518eee81184ced0d2279c769a0eca59aab465c4e7ec13e5e6c47a3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
