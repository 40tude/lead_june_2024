{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e39c69",
   "metadata": {},
   "source": [
    "![](https://uploads-ssl.webflow.com/62233c592d2a1e009d42f46c/6414802c0a2bea367cbc795b_logo-jedha-square.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0cbf2b",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: left; color:#20a08d; font-size: 40px\"><span><strong>Q Learning dans l'environnement Cart Pole\n",
    "</strong></span></h1>\n",
    "\n",
    "Cet exercice se concentrera sur la résolution du problème d'apprentissage par renforcement pour l'environnement Cart Pole.\n",
    "\n",
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>Description\n",
    "</strong></span></h2>\n",
    "\n",
    "Cet environnement correspond à la version du problème du chariot décrit par Barto, Sutton et Anderson dans \"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\". \n",
    "\n",
    "Un pendule est attaché par une articulation non actionnée à un chariot, qui se déplace le long d'une piste sans frottement. Le pendule est placé debout sur le chariot et le but est d'équilibrer le pendule en appliquant des forces dans la direction gauche et droite sur le chariot.\n",
    "\n",
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>Espace d'actions\n",
    "</strong></span></h2>\n",
    "\n",
    "L'action est un vecteur de forme (1,) qui peut prendre des valeurs {0, 1} indiquant la direction de la force fixe avec laquelle le chariot est poussé.\n",
    "\n",
    "The action is a ndarray with shape (1,) which can take values {0, 1} indicating the direction of the fixed force the cart is pushed with.\n",
    "\n",
    "Action:\n",
    "* 0 : Pousser le chariot vers la gauche\n",
    "* 1 : Pousser le chariot vers la droite\n",
    "\n",
    "Remarque : La vitesse qui est réduite ou augmentée par la force appliquée n'est pas fixe et dépend de l'angle vers lequel pointe le pendule. Le centre de gravité du pendule fait varier la quantité d'énergie nécessaire pour déplacer le chariot en dessous\n",
    "\n",
    "\n",
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>Espace d'états\n",
    "</strong></span></h2>\n",
    "\n",
    "L'observation est un vecteur de forme (4,) avec les valeurs correspondant aux positions et vitesses suivantes :\n",
    "\n",
    "Num\n",
    "Observation\n",
    "Min\n",
    "Max\n",
    "\n",
    "* 0: Cart Position $\\in [-4.8, 4.8]$\n",
    "* 1: Cart Velocity $\\in [-\\infty, \\infty]$\n",
    "* 2: Pole Angle $\\in [~ -0.418 rad (-24°), ~ 0.418 rad (24°)]$\n",
    "* 3: Pole Angular Velocity $\\in [-\\infty,\\infty]$\n",
    "\n",
    "Remarque : Bien que les plages ci-dessus indiquent les valeurs possibles pour l'espace d'observation de chaque élément, elles ne reflètent pas les valeurs autorisées de l'espace d'état dans un épisode non terminé. Notamment:\n",
    "\n",
    "- La position x du chariot (index 0) peut prendre des valeurs comprises entre (-4,8, 4,8), mais l'épisode se termine si le panier quitte la plage (-2,4, 2,4).\n",
    "\n",
    "- L'angle polaire peut être observé entre (-.418, .418) radians (ou ±24°), mais l'épisode se termine si l'angle polaire n'est pas dans la plage (-.2095, .2095) (ou ±12°)\n",
    "\n",
    "\n",
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>Récompenses\n",
    "</strong></span></h2>\n",
    "\n",
    "Puisque le but est de maintenir le pendule droit le plus longtemps possible, une récompense de +1 pour chaque pas effectué, y compris le pas de fin d'épisode, est attribuée. Le seuil de récompenses est de 475 pour la v1.\n",
    "\n",
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>Etat de départ\n",
    "</strong></span></h2>\n",
    "Toutes les observations se voient attribuer une valeur uniformément aléatoire en (-0,05, 0,05)\n",
    "\n",
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>Fin de l'épisode\n",
    "</strong></span></h2>\n",
    "L'épisode se termine si l'un des événements suivants se produit :\n",
    "\n",
    "- L'angle du pendule est supérieur à ±12°\n",
    "- La position du chariot est supérieure à ±2,4 (le centre du chariot atteint le bord de l'écran)\n",
    "- La longueur de l'épisode est supérieure à 500 steps(200 pour la v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da44503c",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>1. Nous allons commencer par installer les bibliothèques de gym nécessaires à la simulation de l'environnement</strong></span></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee9a38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install gym\n",
    "#!pip3 install gym[classic_control]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beaf029",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>2. Nous procéderons également aux imports nécessaires\n",
    "</strong></span></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c69c3557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # used for arrays\n",
    "\n",
    "import gym # pull the environment\n",
    "\n",
    "import time # to measure execution time\n",
    "\n",
    "import math # needed for calculations\n",
    "\n",
    "import matplotlib.pyplot as plt # for visualizing\n",
    "\n",
    "import pygame # has some effect on the rendering\n",
    "\n",
    "import matplotlib.animation as animation # for making gifs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9ec3b3",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>3. Créons une variable env utilisant la méthode décrite dans la documentation\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643947bb",
   "metadata": {},
   "source": [
    "Documentation Car Pole : [documentation](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8b79af-467d-4848-a59e-45ba75459e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c540868",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>4. jetez un oeil à l'espace d'actions.\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59714371-9500-4d7f-a0e3-3e9cb99b20ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21c30209",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>5. jetez un oeil à l'espace d'états.\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3230ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e7cbc56",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>6. Réinitialiser l'environnement pour jeter un œil à un état.\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c97e0ee-6635-4d7d-bafc-0364720e75cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd57b269",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>7. Puisque l'environnement est continu dans le temps, nous ferons des gifs pour pouvoir visualiser ce que fait l'agent\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7630e62c",
   "metadata": {},
   "source": [
    "Nous allons suivre quelques étapes pour ce faire :\n",
    "- Réinitialiser l'environnement\n",
    "- Définissez une liste vide appelée `arr`\n",
    "- Définir une variable `done` avec une valeur `False`\n",
    "- Définir une variable `i` avec une valeur `0`\n",
    "- Démarrez une boucle `while` qui se poursuivra tant que `done` est `False` dans la boucle :\n",
    "    - Ajouter la visualisation de l'état actuel de l'environnement à la liste`arr` (render_mode='rgb_array'). Le retour de la méthode render devrait être un tableau numpy.\n",
    "    - Effectuer une action aléatoire pour produire la nouvelle observation (état)\n",
    "    - Afficher la nouvelle observation (état)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6036201c-34b5-4af9-851d-71ead57f6705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c9ea873",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>8. Utilisez la commande suivante pour créer un gif à partir de la liste arr des rendus (render)\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35702705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8520013f",
   "metadata": {},
   "source": [
    "Puisque l'espace d'observation est continu et non discret, nous ne pouvons pas appliquer directement le Q-learning. L'astuce consiste ici à convertir ce problème d'apprentissage par renforcement à état continu en un problème d'apprentissage par renforcement discret en divisant la plage des différentes métriques d'observation en catégories. Faisons cela"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4ef590",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>9. Quelle est la fourchette légale des quatre mesures différentes qui définissent un état non terminal ?\n",
    "</strong></span></h3>\n",
    "Pour faire la fonction de discrétisation cependant, n'hésitez pas à supposer des plages plus grandes pour éviter toute erreur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5a05d9",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>10. Nous aimerions diviser toutes nos variables d'état en 51 catégories, mettre en place un objet Observation  qui est une liste [51,51,51,51]\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffec934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cae45d1a",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>11. Nous devons définir une fonction (nous l'appellerons get_discrete_state) qui transforme une observation d'état continue en une observation d'état discrète. \n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5668c90",
   "metadata": {},
   "source": [
    "L'idée est d'avoir 50 catégories avec la catégorie 0 correspondant à la valeur la plus basse et 50 étant la valeur la plus élevée. L'entrée de la fonction doit être un état et la sortie doit être un tuple d'entiers compris entre 0 et 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574a8970",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>12. Créez la table Q pour le problème d'apprentissage par renforcement discrétisé, initialisez-la avec des zéros. Quelle est sa forme ?\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f15f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a08f0dd3",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>13. Initialisons maintenant les valeurs dont nous aurons besoin pour que l'algorithme Q-learning fonctionne\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0522438",
   "metadata": {},
   "source": [
    "* `LEARNING_RATE` = 0.1\n",
    "* `DISCOUNT` = 0.95\n",
    "* `EPISODES` = 60000\n",
    "* `total` = 0\n",
    "* `total_reward` = 0\n",
    "* `prior_reward` = 0\n",
    "* `epsilon` = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff6da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c9596eb",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>14. Codons maintenant l'algorithme Q-learning, voici les étapes :\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bde2fc",
   "metadata": {},
   "source": [
    "- Boucle sur le nombre d'épisodes\n",
    "- réinitialiser l'environnement\n",
    "- discrétiser l'état initial\n",
    "- initialiser `done=False`\n",
    "- initialiser `episode_reward=0`\n",
    "- Faire une boucle jusqu'à ce que `done` soit `True`\n",
    "    - conditions de mise en place pour mettre en œuvre une politique $\\epsilon-greedy$\n",
    "    - faire un step en réalisant l'action choisie\n",
    "    - incrémenter `episode_reward`\n",
    "    - discrétiser le nouvel état\n",
    "    - si l'état n'est pas terminal :\n",
    "        - mettre à jour la Q-table avec la règle de mise à jour de Q-learning\n",
    "    - remplacer l'état discret actuel par le nouvel état discret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91311f4",
   "metadata": {},
   "source": [
    "En guise de contrôle d'intégrité, affichez la moyenne episode_reward calculée sur les 1 000 derniers épisodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ea39e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afc03365",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>15. Maintenant que l'entrainement est terminé, il semble que les choses se soient très bien passées ! Réutilisez le code pour faire l'animation afin d'afficher le comportement de l'agent entrainé sur le jeu CartPole\n",
    "</strong></span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3fa8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1278cfb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0737bc02",
   "metadata": {},
   "source": [
    "Toutes nos félicitations! Vous avez résolu avec succès un problème d'apprentissage par renforcement avec un espace d'état continu ! La technique de discrétisation est très courante face à un espace d'état continu assez simple comme celui-ci ! Une autre approche consisterait à s'appuyer sur l'approximation de fonctions afin de mapper les états et les actions sur différentes valeurs afin de sélectionner des actions à l'aide d'une fonction (et c'est là que les réseaux de neurones profonds peuvent jouer un rôle important !)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a041ce",
   "metadata": {},
   "source": [
    "NB : Notez que discrétiser un problème comme celui-ci pour utiliser le Q-learning classique est en soi déjà une approximation de fonction ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16+"
  },
  "vscode": {
   "interpreter": {
    "hash": "fbc4d3870518eee81184ced0d2279c769a0eca59aab465c4e7ec13e5e6c47a3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
