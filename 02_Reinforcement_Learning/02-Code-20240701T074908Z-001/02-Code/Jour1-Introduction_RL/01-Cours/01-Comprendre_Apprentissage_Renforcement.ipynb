{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://uploads-ssl.webflow.com/62233c592d2a1e009d42f46c/6414802c0a2bea367cbc795b_logo-jedha-square.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprendre l'apprentissage par renforcement üß†üß†\n",
    "\n",
    "L'apprentissage par renforcement est une sous-section de l'apprentissage automatique qui se concentre sur l'apprentissage par l'exp√©rience plut√¥t que sur l'apprentissage √† partir d'un ensemble de donn√©es pr√©c√©demment donn√©. Il se ramifie en de nombreux types de probl√®mes diff√©rents pour lesquels l'apprenant (souvent appel√© agent) n'est pas explicitement inform√© de ce qu'il doit faire, mais d√©couvre quelles actions rapportent le plus de r√©compense en r√©action √† l'environnement dans lequel il √©volue. Pour imaginer une id√©e g√©n√©rale, pensez √† vous-m√™me apprendre √† jouer √† un jeu vid√©o par essais et erreurs.\n",
    "\n",
    "Avant de commencer, introduisons le vocabulaire suivant qui sera utilis√© tout au long de la conf√©rence :\n",
    "\n",
    "- **environnement** : une situation qui produit des observations compos√©es de :\n",
    "    - **√©tat** : une mesure de ce qui se passe dans l'environnement (position sur une carte, temp√©rature d'un moteur, position d'un jeu d'√©checs etc...)\n",
    "    - **r√©compense** : une m√©trique instantan√©e donn√©e pour atteindre un √©tat sp√©cifique\n",
    "    - **statut** : dans les situations o√π l'exp√©rience peut se terminer (jeu d'arcade), donne le statut de fin\n",
    "- **agent** : l'apprenant en interaction avec l'environnement\n",
    "- **politique** : l'ensemble des r√®gles qui d√©terminent les actions que l'agent entreprendra compte tenu d'un certain √©tat de l'environnement\n",
    "\n",
    "Gardez-les √† l'esprit au fur et √† mesure que nous avan√ßons dans le cours.\n",
    "\n",
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>Qu'allez-vous apprendre dans ce cours ? üßêüßê </strong></span></h2>\n",
    "\n",
    "Le cours couvrira diff√©rents ensembles de probl√®mes et de solutions en passant progressivement du plus simple au plus g√©n√©ral.\n",
    "\n",
    "\n",
    "- Introduction g√©n√©rale\n",
    "- Bandit manchot : environnement √† un seul √©tat\n",
    "- Processus de d√©cision de Markov finis : environnement √† √©tats multiples\n",
    "- Programmation dynamique : Parfaite connaissance de l'environnement\n",
    "- M√©thodes de Monte Carlo : apprendre de l'exp√©rience\n",
    "- Apprentissage par Diff√©rence Temporelle : Apprentissage par l'exp√©rience √† partir d'estimations\n",
    "- R√©sum√©\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>General Introduction üß∏ </strong></span></h2>\n",
    "\n",
    "L'id√©e d'apprendre en interagissant avec l'environnement afin d'atteindre un objectif sp√©cifique est une fa√ßon tr√®s intuitive pour nous de penser √† l'apprentissage. Lorsque les enfants regardent autour d'eux, commencent √† bouger leurs bras et leurs jambes, essaient de se d√©placer dans leur environnement, ils n'ont pas d'enseignant explicite, mais ils ont une perception de l'environnement qui les entoure, et ils peuvent √©galement √©prouver du plaisir et de la douleur comme des cons√©quences de leurs actions\n",
    "\n",
    "L'apprentissage par renforcement est bas√© sur ce principe qu'un apprenant (un ensemble sp√©cifique de r√®gles pour d√©cider quelle action choisir lorsqu'il est confront√© √† une certaine situation/√©tat de l'environnement) doit optimiser une certaine quantit√© qui mesure le niveau de satisfaction qui d√©coule de la prise de ces diff√©rentes actions.\n",
    "\n",
    "Ces derni√®res ann√©es, les progr√®s r√©alis√©s dans la construction de mat√©riel avec plus de puissance de calcul ont permis de porter l'apprentissage par renforcement √† un tout autre niveau gr√¢ce √† l'apprentissage par renforcement profond. Pour ne citer qu'un exemple remarquable, l'algorithme alpha go de DeepMind est d√©sormais capable de surpasser les meilleurs champions humains au jeu de go, ce qui √©tait auparavant consid√©r√© comme impossible en raison de la nature ouverte du jeu et du nombre √©norme de configurations possibles. .\n",
    "\n",
    "Voici divers exemples de situations qui pourraient √™tre trait√©es par l'apprentissage par renforcement :\n",
    "\n",
    "- Un **joueur humain** au casino essaie de maximiser son gain en jouant aux machines √† sous\n",
    "- Une **vache nouveau-n√©e** a du mal √† se lever, quelques minutes plus tard elle court dans le champ\n",
    "- Vous √™tes sur le point de **cuisiner un repas √† la maison**. M√™me cette entreprise apparemment insignifiante se r√©v√®le √™tre une s√©rie complexe de d√©cisions impliquant les ingr√©dients et les outils disponibles dans votre cuisine, examinant votre niveau de motivation et votre faim. Marcher jusqu'au r√©frig√©rateur, l'ouvrir, choisir des ingr√©dients, les atteindre et les saisir, couper, allumer la cuisini√®re, ramasser une po√™le √† frire...\n",
    "- Un **robot d'exploration** sur le sol de Mars qui doit choisir entre explorer plus de terrain, prendre des photos ou retourner √† la base pour se recharger.\n",
    "\n",
    "Ces situations impliquent divers aspects essentiels des probl√®mes d'apprentissage par renforcement tels que :\n",
    "\n",
    "- Observabilit√© de l'environnement\n",
    "- Incertitude sur le retour positif ou n√©gatif de certaines actions\n",
    "- Interaction entre un agent d√©cisionnel et son environnement\n",
    "\n",
    "Au cours de ce premier cours, nous nous concentrerons sur la pr√©sentation du contexte th√©orique dont vous aurez besoin pour comprendre l'apprentissage par renforcement, avant de passer √† la mise en ≈ìuvre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>Bandit machot : environnement √† un seul √©tat ü™ô\n",
    " </strong></span></h2>\n",
    "\n",
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>Probl√®me stationnaire </strong></span></h3>\n",
    "\n",
    "L'aspect le plus important de l'apprentissage par renforcement qui le distingue des autres formes d'apprentissage automatique est qu'il utilise des informations d'entrainement qui √©valuent les actions entreprises, plut√¥t que d'instruire les actions correctes. C'est pourquoi nous avons besoin de la recherche par essai-erreur pour un plan d'action optimal.\n",
    "\n",
    "Pour ce premier exemple, nous d√©finissons une situation o√π l'environnement contient un seul √©tat (on parle de cadre *non associatif*), et √† chaque pas de temps nous avons un ensemble de $n$ actions qui s'offrent √† nous avec une certaine r√©compense.\n",
    "\n",
    "Le **probl√®me du bandit manchot** (appel√© par analogie avec le jeu de machines √† sous), correspond √† une situation o√π vous avez $n$ options d'actions √† entreprendre, apr√®s chaque action, vous recevez une r√©compense tir√©e d'une distribution de probabilit√© stationnaire (ce qui signifie que la distribution ne change pas dans le temps) qui d√©pend de l'action s√©lectionn√©e. Votre objectif est de **maximiser le montant total de r√©compense** que vous obtenez apr√®s avoir jou√© 1000 fois par exemple.\n",
    "\n",
    "Chaque action a une r√©compense attendue (la vraie moyenne de la distribution de probabilit√© inconnue sous-jacente) que nous appellerons **la valeur** de cette action. Nous ne connaissons pas les valeurs des n actions diff√©rentes, cependant nous pouvons calculer des estimations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>Actions gloutonnes </strong></span></h3>\n",
    "\n",
    "A chaque pas de temps, on peut choisir l'action pour laquelle on a la valeur estim√©e la plus √©lev√©e, c'est ce qu'on appelle l' **action gloutonne** . Choisir une action gourmande s'appelle **exploiter** , car vous exploitez les connaissances actuelles que vous avez du probl√®me pour √©clairer vos d√©cisions. Si, √† la place, vous choisissez une autre action, vous **explorez** , ce qui vous permet d'obtenir de meilleures estimations des valeurs associ√©es aux actions.\n",
    "\n",
    "Le compromis entre **l'exploitation et l'exploration** est le suivant : l'exploitation vous am√®ne √† maximiser la r√©compense que vous pouvez obtenir imm√©diatement, tandis que l'exploration peut d√©couvrir de meilleures actions qui rapporteraient plus de r√©compenses √† long terme.\n",
    "\n",
    "Concr√®tement si vous avez deux machines √† sous diff√©rentes $A$, et $B$. $A$ a une distribution de r√©compense uniforme sur l'intervalle $[1,2]$, et $B$ a une distribution de r√©compense uniforme sur l'intervalle $[0,10]$. Vous pouvez commencer par √©tablir deux estimations pour les valeurs de jouer $A$ et $B$ √©gal √† z√©ro. Disons qu'on commence par jouer $A$ and $B$ une fois chacun, on obtient $1.5$ en jouant $A$, et $0.2$ en jouant $B$. Cela nous am√®ne √† la conclusion que l'action gloutonne  est de jouer $A$, ce qui semble mieux √† court terme. Jouer $A$ de mani√®re r√©p√©t√©e nous conduira √† avoir des estimations de plus en plus pr√©cises de la valeur de jouer $A$ lequel est $1.5$. Cependant, nous n'obtenons pas la meilleure r√©compense possible √† long terme car la valeur de $B$ est $5$, ce qui signifie que nous aurions d√ª jouer $B$ tout le long. Si nous avions explor√© de temps en temps au lieu d'exploiter tout le temps, nous aurions r√©alis√© que la valeur de $B$ √©tait en fait sup√©rieur √† celui de $A$, ce qui aurait produit des r√©compenses plus √©lev√©s √† long terme.\n",
    "\n",
    "Nous explorerons quelques solutions simples pour √©quilibrer l'exploration et l'exploitation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>Valeurs d'actions </strong></span></h3>\n",
    "\n",
    "Nous d√©finissons l' estimation de la valeur d'action de l'action $a \\in \\mathcal{A}$ where $\\mathcal{A}$ est l'ensemble de toutes les actions possibles:\n",
    "\n",
    "$$\n",
    "Q_t(a)=\\frac{R_1 + R_2 + \\dots + R_{N_t(a)}}{N_t(a)}\n",
    "$$\n",
    "\n",
    "O√π $R_i$ est la r√©compense re√ßue au $i^{√®me}$ timestep lorsque nous r√©alisons l'action $a$, et $N_t(a)$ est le nombre total de fois que nous avons s√©lectionn√© une action $a$.\n",
    "\n",
    "Ce n'est rien de plus que d'estimer que la valeur de l'action $a$ est la r√©compense moyenne obtenue en choisissant une action $a$. Assez simple, n'est-ce pas ?\n",
    "\n",
    "Nous pouvons commencer par √©tablir une valeur initiale pour $Q_0(a)$ comme $0$ quand $N_t(a)=0$, et as $N_t(a) \\rightarrow \\infty$ alors $Q_t(a) \\rightarrow q(a)$ a vraie valeur d'action de l'actionun $a$ gr√¢ce √† la loi des grands nombres. C'est ce qu'on appelle la m√©thode *sample-average*.\n",
    "\n",
    "La r√®gle de s√©lection d'action la plus simple consiste √† toujours choisir l'une des actions gloutonne, qui v√©rifie :\n",
    "\n",
    "\n",
    "$$\n",
    "Q(A_t^*)=\\max_a Q_t(a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "A_t^* = \\argmax_a Q_t(a)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> $\\epsilon-greedy$ </strong></span></h3>\n",
    "\n",
    "Afin d'√©quilibrer l'exploitation et l'exploration, nous pouvons choisir ce que nous appelons politique $\\epsilon-greedy$  qui donnent une probabilit√© de $1-\\epsilon$ pour s√©lectionner des actions gloutonnes et une faible probabilit√© $\\epsilon$ pour s√©lectionner l'une des autres actions disponibles comme al√©atoire. L'avantage de telles m√©thodes est que $N_t(a) \\rightarrow \\infty$ pour toutes les actions $a\\in A$, ce qui garantit que l'estimation de toutes les valeurs d'action deviendra arbitrairement pr√©cise avec suffisamment d'essais.\n",
    "\n",
    "\n",
    "La figure suivante montre l'√©volution des r√©compenses et la proportion d'actions optimales prises au fil du temps en utilisant diff√©rentes strat√©gies de s√©lection d'action.\n",
    "\n",
    "![multi-arm-bandit-decision](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/LEAD/Reinforcement-Learning/multi-arm-bandit.png)\n",
    "\n",
    "La mise en ≈ìuvre de l'√©valuation des estimations de la valeur d'une action peut se faire de mani√®re incr√©mentale, cette r√®gle de mise √† jour est un paradigme tr√®s courant dans l'apprentissage par renforcement qui peut s'√©crire de mani√®re tr√®s g√©n√©rale comme :\n",
    "\n",
    "$$\n",
    "NewEstimate \\leftarrow OldEstimate + StepSize [Target - OldEstimate]\n",
    "$$\n",
    "\n",
    "Ce qui devrait vous rappeler un peu la logique descente/mont√©e du gradient. Dans le cas de notre m√©thode de moyenne d'√©chantillon de bandit manchot :\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Q_{k+1} & = \\frac{1}{k} \\sum_{i=1}{k}{R_i} \\\\\n",
    "& = \\frac{1}{k} ( R_k + \\sum_{i=1}{k-1}{R_i} ) \\\\\n",
    "& = \\frac{1}{k} ( R_k + (k-1)Q_k + Q_k - Q_k ) \\\\\n",
    "& = \\frac{1}{k} ( R_k + kQ_k - Q_k ) \\\\\n",
    "& = Q_k + \\frac{1}{k} (R_k - Q_k)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "O√π $k$ repr√©sente le $k^{√®me}$ timestep o√π nous avons choisi l'action a. Notez que la taille de pas que nous utilisons ici pour incr√©menter la valeur de l'estimation varie dans le temps. Dans les notations plus g√©n√©rales du probl√®me d'apprentissage par renforcement, nous d√©signerons g√©n√©ralement la taille du pas par $\\alpha$, ou $\\alpha_t$ si la taille du pas d√©pend du temps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> Probl√®me non stationnaire\n",
    " </strong></span></h3>\n",
    "\n",
    "Et si la vraie valeur des actions variait dans le temps ? Alors, l'approche pr√©c√©dente de moyenne d'√©chantillon ne fonctionnerait pas car le poids des nouvelles valeurs de la r√©compense diminuerait avec le temps, donnant l'inertie de l'estimation qui n'est pas adapt√©e √† un probl√®me non stationnaire.\n",
    "\n",
    "Une approche courante pour les probl√®mes non stationnaires consiste √† configurer la r√®gle de mise √† jour suivante :\n",
    "\n",
    "$$\n",
    "Q_{k+1}=Q_k + \\alpha [R_k - Q_k]\n",
    "$$\n",
    "\n",
    "Avec une constante $\\alpha \\in ]0,1]$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Q_{k+1}& = Q_k + \\alpha [R_k - Q_k] \\\\\n",
    "& = \\alpha R_k + (1-\\alpha) Q_k \\\\\n",
    "& = \\alpha R_k + (1-\\alpha) [\\alpha R_{k-1} + (1-\\alpha) Q_{k-1}] \\\\\n",
    "& = \\alpha R_k + (1-\\alpha)\\alpha R_{k-1} + (1-\\alpha)^2 Q_{k-1} \\\\\n",
    "& = \\alpha R_k + (1-\\alpha)\\alpha R_{k-1} + (1-\\alpha)^2 \\alpha R_{k-2} + \\dots + (1-\\alpha)^{k-1}\\alpha R_1 + (1-\\alpha)^{k}Q_1 \\\\\n",
    "& = (1-\\alpha)^{k}Q_1 + \\alpha \\sum_{i=1}^{k}{(1-\\alpha)^{k-i}R_i}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Il s'agit d'une moyenne pond√©r√©e des r√©compenses au fil du temps, o√π les r√©compenses pass√©es re√ßoivent une importance d√©croissante de mani√®re exponentielle au fil du temps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> Perspective sur l'exemple non associatif (bandits manchots contextuels)\n",
    " </strong></span></h3>\n",
    "\n",
    "Le probl√®me classique du bandit manchot est non associatif, c'est-√†-dire que l'environnement auquel vous √™tes confront√© a un seul √©tat (m√™me si les valeurs des diff√©rentes actions peuvent changer). Cet exemple ne cadre pas n√©cessairement bien avec le probl√®me g√©n√©ral d'apprentissage par renforcement pour lequel l'environnement fournit diff√©rents √©tats.\n",
    "\n",
    "Vous pourriez imaginer une situation o√π vous avez deux probl√®mes diff√©rents de bandit multi-bras, et √† chaque fois que vous passez de l'un √† l'autre vous obtenez un signal, par exemple vert pour l'un et rouge pour l'autre, cette perception de l'environnement s'appelle l' **√©tat** , not√© g√©n√©ralements $s\\in \\mathcal{S}$, o√π $\\mathcal{S}$ est l'ensemble des √©tats possibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>Processus de d√©cision de Markov finis : environnement √† √©tats multiples ‚õìÔ∏è\n",
    " </strong></span></h2>\n",
    "\n",
    "Maintenant que vous comprenez certains des enjeux du probl√®me d'apprentissage par renforcement, essayons de d√©finir un cadre plus g√©n√©ral pour formaliser le probl√®me d'un **agent** interagissant avec un **environnement** pour atteindre un **but** .\n",
    "\n",
    "La figure suivante donne une illustration simple de la fa√ßon dont nous pouvons visualiser une telle situation :\n",
    "\n",
    "![environment-agent](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/LEAD/Reinforcement-Learning/environment_agent_interface.png)\n",
    "\n",
    "A chaque pas de temps, l'agent re√ßoit un signal de l'environnement sous la forme d'un √©tat $S_t \\in \\mathcal{S}$ avec une r√©compense $R_t$, puis choisit une action $A_t \\in \\mathcal{A}(S_t)$ ce qui donne un nouvel √©tat $S_{t+1}$ et une nouvelle r√©compense $R_{t+1} \\in \\mathcal{R} \\subset \\mathbb{R}$. \n",
    "\n",
    "A chaque pas de temps, l'agent impl√©mente un mapping de l'√©tat √† la probabilit√© de s√©lectionner chaque action possible, ce mappage est appel√© une **politique** et est not√©\\pi_t $\\pi_t$, o√π $\\pi_t(a|s)$ iest la probabilit√© de choisir une action $A_t=a$ connaissant $S_t=s$.\n",
    "\n",
    "Les m√©thodes d'apprentissage par renforcement d√©crivent comment l'agent met √† jour sa politique afin de maximiser ses r√©compenses sur le long terme.\n",
    "\n",
    "Cette repr√©sentation peut √™tre utilis√©e pour repr√©senter un grand nombre de situations d'apprentissage diff√©rentes. Attention cependant que la fronti√®re entre l'agent et l'environnement n'est pas n√©cessairement dessin√©e comme la limite du \"corps physique\" de l'agent, en pratique l'agent encapsule tout ce qui peut √™tre arbitrairement influenc√© par l'agent, tout ce sur quoi l'agent n'a pas un contr√¥le absolu et complet est l'environnement. Cette limite n'a rien √† voir avec la quantit√© de connaissances que l'agent poss√®de sur l'environnement, par exemple m√™me si vous savez exactement comment fonctionne un jeu de Rubik's cube, vous ne pourrez peut-√™tre toujours pas le r√©soudre.\n",
    "\n",
    "Dans ce qui suit, nous d√©finissons la r√©compense cumul√©e $G_t$ comme la somme escompt√©e des r√©compenses √† partir du pas de temps $t$ :\n",
    "\n",
    "$$\n",
    "G_t = \\sum_{k=0}^{T-t-1}{\\gamma^{k}R_{t+k+1}}\n",
    "$$\n",
    "\n",
    "Y compris le cas o√π $T=\\infty$ pour les **t√¢ches continues** et √©ventuellement $\\gamma=1$ pour certains cas de **t√¢ches √©pisodiques** .\n",
    "\n",
    "- **T√¢ches √©pisodiques** : T√¢ches qui peuvent se terminer et √™tre r√©p√©t√©es √† partir de z√©ro, comme un jeu d'arcade.\n",
    "- **T√¢ches continues** : Suivi des r√©glages d'un moteur pour une machine de fabrication qui ne s'arr√™te pas.\n",
    "\n",
    "En pratique, des parall√®les √©troits peuvent √™tre √©tablis entre les t√¢ches continues et √©pisodiques, nous utiliserons donc les m√™mes notations pour les deux types de probl√®mes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> La propri√©t√© de Markov\n",
    " </strong></span></h3>\n",
    "\n",
    "Dans le cadre de l'apprentissage par renforcement, l'agent fonde sa d√©cision sur l'√©tat fourni par l'environnement, qui est l'ensemble des informations auxquelles l'agent peut acc√©der √† un pas de temps donn√©. La propri√©t√© de Markov indique que le signal d'√©tat actuel contient toutes les informations pertinentes dont l'agent a besoin pour d√©cider de l'action √† entreprendre, quels que soient les √©tats ant√©rieurs que l'agent a connus dans le pass√©.\n",
    "\n",
    "Formellement parlant, la propri√©t√© de Markov peut s'√©crire comme suit :\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(R_{t+1}=r, S_{t+1}=s'|S_0,A_0,R_1,\\dots,R_{t-1},S_{t-1},A_{t-1},S_t,A_t) = \\mathbb{P}(R_{t+1}=r, S_{t+1}=s'|S_t,A_t)\n",
    "$$\n",
    "\n",
    "Si un environnement a la propri√©t√© de Markov, la r√©compense attendue et l'√©tat suivant sont enti√®rement donn√©s par l'√©tat actuel et l'action actuelle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> Processus de d√©cision de Markov\n",
    "</strong></span></h3>\n",
    "\n",
    "Toute t√¢che d'apprentissage par renforcement qui a la propri√©t√© de Markov est un processus de d√©cision de Markov ou MDP. Un processus de d√©cision de Markov fini est un processus de d√©cision de Markov avec un √©tat et un espace d'action finis.\n",
    "\n",
    "√âtant donn√© n'importe quel √©tat et action $s$ and $a$ dans l'ensemble d'√©tat\\mathcal $\\mathcal{S}$ et ensemble d'actions $\\mathcal{A}(s)$, la probabilit√© de chaque paire suivante d'√©tat et de r√©compense $s'$ and $r$ is:\n",
    "\n",
    "$$\n",
    "p(s',r|s,a)=\\mathbb{P}(S_{t+1}=s',R_{t+1}=r|S_t=s,A_t=a)\n",
    "$$\n",
    "\n",
    "Ces quantit√©s d√©finissent compl√®tement la dynamique d'un MDP fini.\n",
    "\n",
    "On peut alors d√©finir la r√©compense cumul√©e esp√©r√©e :\n",
    "\n",
    "\n",
    "$$\n",
    "r(s,a) = \\mathbb{E}(R_{t+1}|S_t=s,A_t=a)=\\sum_{r\\in \\mathcal{R}}{r}\\sum_{s'\\in \\mathcal{S}}{p(s',r|s,a)}\n",
    "$$\n",
    "\n",
    "La probabilit√© de transition d'√©tat :\n",
    "\n",
    "\n",
    "$$\n",
    "p(s'|s,a) = \\mathbb{P}(S_{t+1}|S_t=s,A_t=a)=\\sum_{r\\in \\mathcal{R}}{p(s',r|s,a)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> Fonctions de valeur\n",
    "</strong></span></h3>\n",
    "\n",
    "Dans l'apprentissage par renforcement, nous essayons souvent d'√©valuer des **fonctions √©tat-valeur** , qui estiment √† quel point il est bon pour l'agent d'√™tre dans un √©tat donn√©, ou des fonctions **√©tat-action valeur** qui estiment √† quel point il est bon d'effectuer une action donn√©e tout en √©tant dans un √©tat donn√©. √âvidemment, les r√©compenses futures auxquelles un agent peut s'attendre d√©pendent des diff√©rentes actions qu'il va choisir, donc les fonctions d'√©tat-valeur et les fonctions d'√©tat-valeur d'action sont d√©finies par rapport √† une politique donn√©e \\$\\pi$ qui mappe la propri√©t√© de chaque action en fonction d'un certain √©tat.\n",
    "\n",
    "Formellement, nous d√©finissons la fonction √©tat-valeur par rapport √† la politique $\\pi$ √† partir d'un √©tat $s$ comme $v_{\\pi}(s)$:\n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) = \\mathbb{E}_{\\pi}(G_t|S_t=s) = \\mathbb{E}(\\sum_{k=0}^{T-t-1}{\\gamma^k R_{t+k+1}} | S_t=s)\n",
    "$$\n",
    "\n",
    "o√π $\\mathbb{E}_{\\pi()}$ est la valeur attendue d'une variable al√©atoire √©tant donn√© que l'agent suit la politique $\\pi$.\n",
    "\n",
    "De m√™me, nous d√©finissons la fonction action-valeur pour la politique $\\pi$, $q_{\\pi}(s,a)$ de choisir l'action $a$ dans l√©tat $s$:\n",
    "\n",
    "$$\n",
    "q_{\\pi}(s,a)=\\mathbb{E}_{\\pi}(G_t | S_t=s, A_t=a) = \\mathbb{E}(\\sum_{k=0}^{T-t-1}{\\gamma^k R_{t+k+1}} | S_t=s, A_t=a)\n",
    "$$\n",
    "\n",
    "En pratique, nous pouvons estimer les deux fonctions de valeur √† partir de l'exp√©rience en enregistrant les r√©compenses cumul√©es moyens obtenus apr√®s avoir √©t√© dans chaque √©tat et pris toutes les mesures possibles (puisque la moyenne obtenue √† partir de l'exp√©rience pour une variable al√©atoire converge vers sa valeur attendue compte tenu d'un nombre suffisant d'essais) ces m√©thodes de simulation sont appel√©es **m√©thodes de Monte Carlo** ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> √âquation de Bellman\n",
    "</strong></span></h3>\n",
    "\n",
    "L'√©quation de Bellman, fondamentale pour l'apprentissage par renforcement, donne l'expression de la fonction valeur  $v_{\\pi}$ en fonction de ses √©tats futurs possibles. Il est au c≈ìur du calcul, de l'approximation et de l'apprentissage de $v_\\pi$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s)& = \\mathbb{E}_{\\pi}(G_t|S_t=s)\\\\\n",
    "& = \\mathbb{E}_{\\pi}(\\sum_{k=0}^{T-t-1}{\\gamma^k R_{t+k+1}} | S_t=s)\\\\\n",
    "& = \\mathbb{E}_{\\pi}(R_{k+1} + \\gamma\\sum_{k=0}^{T-t-1}{\\gamma^k R_{t+k+2}} | S_t=s) \\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s'\\in\\mathcal{S},r\\in\\mathcal{R}}{{p(s',r|s,a)}}[r + \\gamma\\mathbb{E}_{\\pi}(\\sum_{k=0}^{T-t-1}{\\gamma^k R_{t+k+2}} | S_{t+1}=s')]\\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s'\\in\\mathcal{S},r\\in\\mathcal{R}}{p(s',r|s,a)}[r + \\gamma v_{\\pi}(s')]\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> Fonction de valeur optimale\n",
    "</strong></span></h3>\n",
    "\n",
    "En pratique, nous souhaitons trouver la fonction de valeur optimale, ce qui signifie en r√©alit√© trouver l'une des politiques de choix des actions dans un √©tat donn√© qui maximise la r√©compense cumul√©e attendue √† partir d'un √©tat de d√©part donn√©. On d√©finit la fonction **√©tat-valeur optimale** $v_*$ as:\n",
    "\n",
    "$$\n",
    "v_*(s)=\\max_{\\pi}{v_{\\pi}(s)} \\; \\forall s \\in \\mathcal{S}\n",
    "$$\n",
    "\n",
    "La politique optimale vous donne √©galement la **fonction de valeur √©tat-action** optimale * $q_*$ comme √©tant:\n",
    "\n",
    "$$\n",
    "q_*(s,a) = \\max_{\\pi}{q_{\\pi}(s,a)} \\; \\forall s \\in \\mathcal{S}, \\forall a in \\mathcal{A}(s)\n",
    "$$\n",
    "\n",
    "Ainsi, nous pouvons √©crire la fonction de valeur √©tat-action optimale en termes de valeur attendue de prendre $a$ et suivant la politique optimale par la suite:\n",
    "\n",
    "$$\n",
    "q_*(s,a) = \\mathbb{E}(R_{t+1} + \\gamma v_*(S_{t+1})|S_{t}=s, A_t = a)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> Approximation\n",
    "</strong></span></h3>\n",
    "\n",
    "Bien que dans la plupart des cas, lorsque l'espace d'√©tats et l'espace d'action sont finis, il est possible de **trouver exactement la solution √† partir de l'√©quation de Bellman** et de trouver la **politique optimale** et les fonctions de valeur. Cependant, en pratique, la solution analytique peut √™tre tr√®s difficile √† calculer exactement √† cause des contraintes de puissance de calcul. Un jeu comme les √©checs repr√©sente un tr√®s petit sous-ensemble de la complexit√© de l'exp√©rience humaine, et bien que nous ayons une connaissance compl√®te du fonctionnement de l'environnement (les r√®gles du jeu), le nombre d'√©tats et de mouvements possibles n√©cessiterait trop de puissance de calcul pour le ordinateur pour en d√©duire la politique optimale.\n",
    "\n",
    "Notre cadrage du probl√®me d'apprentissage par renforcement nous oblige √† nous contenter de l'approximation, mais nous avons de grandes opportunit√©s pour faire des approximations utiles de la politique optimale ! Et c'est exactement l√† que toute la datascience que nous avons apprise jusqu'√† pr√©sent entre en jeu !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> Exemple : robot de recyclage\n",
    "</strong></span></h3>\n",
    "\n",
    "Imaginez un robot dont le but est de ramasser des canettes vides dans un environnement de bureau. Le processus de d√©cision concernant la recherche de canettes vides dans le bureau est contr√¥l√© par un agent d'apprentissage par renforcement. L'√©tat indique le niveau de la batterie du robot qui peut √™tre faible ou √©lev√©. Nous d√©finissons l'espace d'action du robot comme √©tant :\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{A}(low)&=\\{search,wait,recharge\\}\\\\\n",
    "\\mathcal{A}(high)&=\\{search,wait\\}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Les diff√©rentes probabilit√©s de transition d'√©tat et les r√©compenses de transition d'√©tat sont donn√©es par le tableau suivant :\n",
    "\n",
    "\n",
    "![robottable](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/LEAD/Reinforcement-Learning/rcycling-robot-table.png)\n",
    "\n",
    "Nous pouvons √©galement tracer un diagramme pour les probabilit√©s de transition d'√©tat et les r√©compenses, repr√©sentant ainsi le probl√®me du robot de recyclage comme un processus de d√©cision de markov fini :\n",
    "\n",
    "![robotschema](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/LEAD/Reinforcement-Learning/recycling-robot-schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>Programmation dynamique : Parfaite connaissance de l'environnement üèÉ‚Äç‚ôÄÔ∏è\n",
    " </strong></span></h2>\n",
    " \n",
    " La programmation dynamique repr√©sente une collection d'algorithmes pour approximer les solutions optimales pour les probl√®mes d'apprentissage par renforcement √©tant donn√© un mod√®le parfait de l'environnement. L'hypoth√®se d'un mod√®le parfait de l'environnement et la grande puissance de calcul n√©cessaire √† la programmation dynamique rendent ces algorithmes d'une utilit√© limit√©e dans les applications concr√®tes, bien qu'ils soient encore tr√®s importants d'un point de vue th√©orique.\n",
    "\n",
    "Supposez que nous avons un mod√®le parfait de l'environnement signifie que nous connaissons toutes les probabilit√©s :\n",
    "\n",
    "$$\n",
    "p(s',r|s,a) \\: \\forall s,s' \\in \\mathcal{S} \\: \\forall a \\in \\mathcal{A}(s) \\: \\forall r \\in \\mathcal{R}\n",
    "$$\n",
    "\n",
    "On suppose aussi que le MDP est fini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> Policy evaluation\n",
    "</strong></span></h3>\n",
    "\n",
    "Afin de pouvoir trouver une politique optimale, nous devons pouvoir l'√©valuer par des techniques d'approximation. Pour ce faire, nous pouvons utiliser la r√®gle de mise √† jour de l'√©quation de Bellman :\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "v_{k+1}(s)& = \\mathbb{E}_{\\pi}(R_{t+1} + \\gamma v_k(S_{t+1})|S_t=s)\\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s'\\in\\mathcal{S},r\\in\\mathcal{R}}{p(s',r|s,a)}[r + \\gamma v_{k}(s')]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Cette s√©quence de fonctions de valeur converge vers la fonction √©tat-valeur $v_{\\pi}$\n",
    "\n",
    "L'algorithme d'approximation de la fonction valeur peut s'√©crire comme suit :\n",
    "\n",
    "![policy_eval](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/LEAD/Reinforcement-Learning/policy_eval.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> Am√©lioration de la politique\n",
    "</strong></span></h3>\n",
    "\n",
    "\n",
    "La raison de vouloir √©valuer les politiques est de pouvoir trouver de meilleures politiques ! Consid√©rons que nous avons √©valu√© la fonction de valeur $v_{\\pi}$ pour une politique $\\pi$.\n",
    "\n",
    "Pour un √©tat donn√© $s$ nous aimerions savoir si nous devrions changer la politique $\\pi$ choisir de mani√®re d√©terministe une action alternative $a \\neq \\pi(s)$, puis recommencez √† vous comporter conform√©ment √† la politique $\\pi$. Ce nouveau comportement conduit √† la valeur state-action:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s,a) & = \\mathbb{E}_{\\pi}(R_{t+1} + \\gamma v_{\\pi}(S_{t+1}| S_t=s, A_t=a))\\\\\n",
    "& = \\sum_{s'\\in \\mathcal{S},r\\in \\mathcal{R}}{p(s',r|s,a)(r+\\gamma v_{\\pi}(s'))}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Si la valeur que nous obtenons est sup√©rieure √† $v_{\\pi}(s)$ alors il vaudrait mieux choisir l'action $a$ √† chaque fois que nous sommes dans l'√©tat s, ce qui signifierait que la nouvelle politique est globalement meilleure que la politique actuelle pour tous les √©tats $s\\in \\mathcal{S}$. Cette affirmation est en fait un cas particulier du **th√©or√®me d'am√©lioration de la politique**:\n",
    "\n",
    "Soient $\\pi$ et $\\pi'$ deux politiques diff√©rentes telles que pour tout $s\\in \\mathcal{S}$:\n",
    "\n",
    "$$\n",
    "q_{\\pi}(s, \\pi'(s)) \\geq v_{\\pi}(s)\n",
    "$$\n",
    "\n",
    "Alors la politique $\\pi'$ doit √™tre aussi bon ou meilleur que $\\pi$ :\n",
    "\n",
    "$$\n",
    "v_{\\pi'} \\geq v_{\\pi}\n",
    "$$\n",
    "\n",
    "Si la premi√®re in√©galit√© est stricte alors la seconde l'est aussi !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> Policy Iteration\n",
    "</strong></span></h3>\n",
    "\n",
    "Cette am√©lioration de la politique nous aide √† mettre en place un algorithme qui peut alterner entre l'√©valuation de la politique et les √©tapes d'am√©lioration de la politique afin de cr√©er des politiques progressivement meilleures. Voici l'algorithme en pseudo code :\n",
    "\n",
    "![policy_improvement](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/LEAD/Reinforcement-Learning/policy_improvement.png)\n",
    "\n",
    "L'id√©e est assez simple, on choisit une politique initiale, puis on l'√©value, ensuite on essaie pour chaque √©tat disponible si on peut trouver une autre action qui donnerait un meilleur rendement que la politique actuelle, on met √† jour la politique en cons√©quence et on continue jusqu'√† ce qu'un la politique optimale est trouv√©e !\n",
    "\n",
    "Comme vous pouvez d√©j√† l'anticiper, ce processus n√©cessite beaucoup de puissance de calcul et souffre √©galement de la mal√©diction de la dimensionnalit√© (la quantit√© de calcul n'augmente pas de mani√®re lin√©aire dans la taille de l'espace d'√©tats ni dans la taille de l'espace d'action).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> Programmation dynamique asynchrone\n",
    "</strong></span></h3>\n",
    "\n",
    "L'id√©e de la programmation dynamique asynchrone est d'√©viter d'avoir √† balayer tous les √©tats possibles √† chaque it√©ration de l'algorithme d'it√©ration de la politique, cela √©vite de se retrouver bloqu√© dans des calculs insolubles. Pour de tels algorithmes, les valeurs de certains √©tats peuvent √™tre sauvegard√©es plusieurs fois avant que d'autres √©tats ne soient sauvegard√©s une seule fois.\n",
    "\n",
    "Une fa√ßon d'y parvenir est d'alterner l'am√©lioration des politiques pour un √©tat et l'√©valuation.\n",
    "\n",
    "Ce type de technique permet d'entrem√™ler l'algorithme d'am√©lioration de la politique avec l'interaction en temps r√©el de l'agent avec l'environnement. Nous sauvegardons les √©tats au fur et √† mesure que l'agent les visite, ce qui nous permet √©galement de nous concentrer davantage sur les √©tats que l'agent est le plus susceptible de visiter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> Generalized policy iteration\n",
    "</strong></span></h3>\n",
    "\n",
    "Nous appelons **it√©ration de politique g√©n√©ralis√©e** tout algorithme qui fait interagir les processus d'√©valuation et d'am√©lioration des politiques. Cela d√©crit bien ce qui se passe lorsque l'on essaie de r√©soudre des probl√®mes d'apprentissage par renforcement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> Exemple : Gridworld\n",
    "</strong></span></h3>\n",
    "\n",
    "Gridworld est un exemple typique de probl√®mes d'apprentissage par renforcement, l'agent commence √† une position al√©atoire sur une grille et son objectif est d'atteindre le carr√© gris en aussi peu de mouvements que possible. Pour chaque √©tat, les actions possibles sont, gauche, droite, haut et bas qui am√®nent de mani√®re d√©terministe l'agent √† atteindre la case suivante dans la direction indiqu√©e (sauf lorsqu'il heurte un mur, auquel cas l'agent restera sur la case qu'il c'√©tait avant). \n",
    "\n",
    "\n",
    "La r√©compense obtenue √† chaque transition d'√©tat est $-1$. Nous avons donc une parfaite connaissance de l'environnement, ce qui cr√©e une application parfaite pour la programmation dynamique !\n",
    "\n",
    "![gridworld](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/LEAD/Reinforcement-Learning/gridworld.png)\n",
    "\n",
    "La figure ci-dessous montre l'algorithme d'√©valuation de la politique en jeu lorsqu'il est appliqu√© √† la politique uniforme (toutes les actions sont √©quiprobables), les figures de droite montrent l'√©volution de la politique gourmande correspondant aux valeurs mises √† jour donn√©es par l'algorithme d'√©valuation de la politique.\n",
    "\n",
    "![gridworld-eval-policy](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/LEAD/Reinforcement-Learning/gridword-policy_eval.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>M√©thodes de Monte Carlo : apprendre de l'exp√©rience ‚öóÔ∏è </strong></span></h2>\n",
    " \n",
    "Ici on ne suppose pas une connaissance parfaite de l'environnement, il suffit que l'agent exp√©rimente des interactions avec l'environnement.\n",
    "\n",
    "Il est possible d'apprendre de l'exp√©rience **r√©elle** , ce qui signifie que nous pouvons confronter notre agent √† l'environnement. Ou apprendre de l'exp√©rience **simul√©e** , ce qui signifie que nous aurions besoin d'un mod√®le de l'environnement, bien que la distribution de probabilit√© compl√®te ne soit pas n√©cessaire dans la pratique.\n",
    "\n",
    "La principale caract√©ristique des m√©thodes de Monte Carlo est de r√©soudre le probl√®me d'apprentissage par renforcement en faisant la moyenne des r√©compenses cumul√©es d'√©chantillons. Les valeurs et les changements de politique ne se produisent qu'√† la fin de chaque √©pisode, ce qui n√©cessite de supposer que la t√¢che est √©pisodique et que chaque √©pisode se termine, quelles que soient les actions entreprises.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> Monte Carlo policy evaluation\n",
    "</strong></span></h3>\n",
    "\n",
    "Nous pr√©sentons ici l'algorithme de pr√©diction de Monte Carlo qui permet d'√©valuer une politique :\n",
    "\n",
    "![mc-pred](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/LEAD/Reinforcement-Learning/montecarlo-prediction.png)\n",
    "\n",
    "L'id√©e est que nous ex√©cutons l'exp√©rience en utilisant la politique pour √©valuer et stocker la valeur de r√©compense obtenu apr√®s la premi√®re occurrence de chaque √©tat rencontr√©, puis nous calculons la moyenne de ces r√©compenses qui devraient converger vers la valeur r√©elle de chaque √©tat pour cet politique sp√©cifique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> Monte Carlo policy evaluation\n",
    "</strong></span></h3>\n",
    "\n",
    "### Exemple : Blackjack\n",
    "\n",
    "Le but du jeu de casino populaire du blackjack est d'obtenir des cartes dont la somme des valeurs num√©riques est la plus grande possible **sans d√©passer 21**. Toutes les cartes faciales comptent pour 10, et un as peut compter soit pour 1, soit pour 11. Nous consid√©rons la version en o√π chaque joueur affronte ind√©pendamment le croupier. Le jeu commence avec deux cartes distribu√©es au croupier et au joueur. L'une des cartes du croupier est face visible et l'autre face cach√©e. Si le joueur a 21 imm√©diatement (un as et une carte 10), cela s'appelle un naturel. Il gagne alors √† moins que le croupier ait √©galement un naturel, auquel cas le jeu est un match nul. Si le joueur n'a pas de naturel, alors il peut demander des cartes suppl√©mentaires, une par une (hits), jusqu'√† ce qu'il s'arr√™te (sticks) ou d√©passe 21 (goes bust). S'il fait faillite, il perd; s'il colle, alors c'est au tour du croupier. Le croupier frappe ou colle selon une strat√©gie fixe sans choix : il colle sur n'importe quelle somme de 17 ou plus, et frappe autrement. Si le croupier fait faillite, alors le joueur gagne ; sinon, le r√©sultat - victoire, d√©faite ou match nul - est d√©termin√© par la somme finale qui est la plus proche de 21.\n",
    "\n",
    "\n",
    "Nous pouvons ex√©cuter un nombre infini de tours de jeu avec une politique donn√©e (l'√©tat √©tant le nombre de points en main) pour √©valuer la valeur de chaque √©tat m√™me si nous ne savons rien des probabilit√©s de transition d'√©tat ni des probabilit√©s de r√©compenses possibles donn√©e par l'environnement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> Monte Carlo Control\n",
    "</strong></span></h3>\n",
    "\n",
    "L'id√©e du **contr√¥le de Monte Carlo** consiste √† utiliser l'approximation de Monte Carlo pour les paires d'action d'√©tat afin de choisir la politique optimale pour le probl√®me √† r√©soudre. Il y a g√©n√©ralement deux mani√®res de proc√©der, la premi√®re consiste √† prendre au hasard un √©tat initial (appel√© Exploring Starts), ce qui est assez peu pratique dans une situation o√π nous apprenons de l'exp√©rience r√©elle que nous ne pouvons pas n√©cessairement simuler arbitrairement. L'autre technique, que nous verrons plus en d√©tail, est le contr√¥le Monte Carlo sans d√©buts d'exploration, ce qui signifie que nous devons inclure certaines propri√©t√©s non gourmandes dans notre politique afin de garantir l'exploration √† chaque √©pisode.\n",
    "\n",
    "La figure ci-dessous repr√©sente l'algorithme de contr√¥le Monte Carlo sans explorer les d√©parts en pseudo code :\n",
    "\n",
    "![mccwoes](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/LEAD/Reinforcement-Learning/montecarlo-control-woexploringstart.png)\n",
    "\n",
    "Nous utilisons l'√©valuation de Monte Carlo pour chaque paire d'action d'√©tat apparaissant dans l'√©pisode, ce qui nous aide √† cr√©er une estimation de la fonction de valeur d'action d'√©tat pour cette paire d'action d'√©tat. Ensuite, pour chaque √©tat de l'√©pisode, nous modifions la politique pour choisir avec une probabilit√© √©lev√©e l'action menant √† la r√©compense la plus √©lev√©e, et attribuons de faibles probabilit√©s aux actions sous-optimales (dans l'exemple ici, nous avons choisi une polique $\\epsilon-greedy$).\n",
    "\n",
    "Notez que cette strat√©gie est appel√©e *on-policy* car nous sommes en mesure de g√©n√©rer de nouveaux √©pisodes apr√®s avoir chang√© la politique utilis√©e pour choisir les actions des √©tats, cependant si nous n'avons acc√®s qu'aux √©pisodes produits √† l'aide d'une autre politique $\\mu \\neq \\pi$, $\\mu$ √©tant la politique √† partir des observations, et $\\pi$ la politique que nous souhaitons √©valuer, alors il est possible d'√©valuer $\\pi$ toujours en utilisant une √©valuation hors politique via un √©chantillonnage d'importance. Cette technique compare la probabilit√© de conna√Ætre des trajectoires d'√©pisodes entre les deux politiques diff√©rentes et l'utilise pour calculer la valeur de la politique non observ√©e.\n",
    "\n",
    "L'un des principaux inconv√©nients des m√©thodes de Monte Carlo est qu'elles doivent attendre la fin d'un √©pisode pour commencer √† faire des estimations de la fonction de valeur d'√©tat, ou des fonctions de valeur d'√©tat d'action, et finalement trouver des politiques optimales, qui si vous pensez √† la le cas d'un jeu d'√©checs, ou d'un jeu de go, ou de jouer √† super mario bros sur un super nintendo peut n√©cessiter beaucoup de puissance de calcul et de temps d'attente. L'ensemble des techniques que nous allons explorer maintenant s'affranchit de cette contrainte et repr√©sente ce qui a fait le plus grand bond en avant de l'apprentissage par renforcement ces derni√®res ann√©es."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>Apprentissage par Diff√©rence Temporelle : Apprendre par l'exp√©rience √† partir d'estimations ‚è∞\n",
    " </strong></span></h2>\n",
    "\n",
    "L'apprentissage par diff√©rence temporelle nous permet d'apprendre de l'exp√©rience sans la n√©cessit√© d'un mod√®le de l'environnement (qui jusqu'√† pr√©sent est identique √† l'approche de Monte Carlo), cependant, ce nouvel ensemble de techniques nous permet d'apprendre des estimations pour nos fonctions de valeur bas√©es sur d'autres estimations ( nous appelons g√©n√©ralement cela bootstrap, le lien avec l'√©chantillonnage bootstrap est qu'en statistique, l'√©chantillon d'origine est d√©j√† une estimation de la population globale, tirer de nouveaux √©chantillons √† partir de l√† est √©galement appel√© bootstrap)\n",
    "\n",
    "L'apprentissage par diff√©rence temporelle nous permet de faire des estimations et des mises √† jour apr√®s chaque pas de temps de l'exp√©rience, et non √† la fin de chaque √©pisode.\n",
    "\n",
    "Mise √† jour de Monte Carlo pour les estimations de retour (la cible est le retour de l'√©pisode apr√®s l'√©tat $S_t$):\n",
    "\n",
    "$$\n",
    "V(S_t)\\leftarrow V(S_t) + \\alpha [G_t - V(S_t)]\n",
    "$$\n",
    "\n",
    "Apprentissage par diff√©rence temporelle (la cible est $R_{t+1} + \\gamma V(S_{t})$):\n",
    "\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]\n",
    "$$\n",
    "\n",
    "Les mises √† jour de la diff√©rence temporelle sont inspir√©es directement de l'√©quation de Bellman que nous avons apprise auparavant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> TD policy evaluation\n",
    "</strong></span></h3>\n",
    "\n",
    "Une illustration en pseudo-code de l'√©valuation de la politique de diff√©rence temporelle pourrait s'√©crire comme suit :\n",
    "\n",
    "\n",
    "![TDeval](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/LEAD/Reinforcement-Learning/TDeval.png)\n",
    "\n",
    "Avec cette logique, toutes les valeurs d'√©tat sont initialis√©es, puis la mise √† jour de chaque valeur d'√©tat est mise √† jour √† l'aide de la r√©compense observ√©e et de l'estimation actuelle de la valeur d'√©tat de l'√©tat suivant obtenu apr√®s avoir choisi l'actionununde la politique. Cette technique s'adapte bien aux probl√®mes avec de longs √©pisodes ou des t√¢ches continues car nous n'avons pas besoin d'attendre les r√©sultats finaux r√©els pour commencer √† faire des estimations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> TD policy improvement\n",
    "</strong></span></h3>\n",
    "\n",
    "D√©sormais, afin d'am√©liorer nos politiques, nous devons nous concentrer sur les paires d'actions d'√âtat et sur la mani√®re dont elles passent l'une √† l'autre au lieu de nous concentrer uniquement sur les √âtats.\n",
    "\n",
    "Nous proposons cette illustration de pseudo-code pour l'am√©lioration des politiques √† l'aide de la m√©thode des diff√©rences temporelles :\n",
    "\n",
    "\n",
    "![TDimprove](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/LEAD/Reinforcement-Learning/TDpolicyimprovement.png)\n",
    "\n",
    "Les valeurs d'√©tat-action sont mises √† jour √† l'aide d'estimations d'autres valeurs d'√©tat-action. Il s'agit d'une application sur politique car nous avons besoin de la politique progressivement am√©lior√©e pour pouvoir interagir avec l'environnement afin d'observer les √©tats et les r√©compenses ult√©rieurs apr√®s chaque √©tape de l'exp√©rience. Cette technique est souvent appel√©e SARSA car nous avons besoin du quintuple des √©v√©nements $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$ se produire afin de mettre √† jour la valeur de la paire d'√©tats d'action $S_t, A_t$. \n",
    "\n",
    "Cet algorithme exige √©galement que nous laissions une certaine marge d'exploration dans le choix de notre politique, sinon la politique restera bloqu√©e dans la premi√®re trajectoire plut√¥t bonne qu'elle a trouv√©e, c'est pourquoi l'algorithme sugg√®re que nous utilisions des politiques $\\epsilon-greedy$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong> Q-Learning (off-policy TD-learning)\n",
    "</strong></span></h3>\n",
    "\n",
    "Q learning est l'une des avanc√©es majeures de l'apprentissage par renforcement. Il se base sur la formule de mise √† jour suivante :\n",
    "\n",
    "$$\n",
    "Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha [R_{t+1} + \\gamma \\max_{a\\in \\mathcal{A}(S_t)}Q(S_{t+1},a) - Q(S_t,A_t)]\n",
    "$$\n",
    "\n",
    "Dans ce cas, la fonction de valeur d'action apprise se rapproche directement de la fonction de valeur d'action optimale (et donc de la politique optimale) pour le probl√®me √† r√©soudre, ind√©pendamment de la politique r√©elle suivie pour la prise de d√©cision.\n",
    "\n",
    "La principale diff√©rence avec la m√©thode d'am√©lioration de la politique pr√©c√©dente est qu'au lieu de nous baser sur la prochaine fonction de valeur d'√©tat-action choisie par la politique actuelle afin de mettre √† jour notre estimation, nous utilisons l'estimation de la fonction de valeur d'√©tat-action pour le meilleur prochain coup possible (notez qu'il s'agit toujours d'une estimation). Nous mettons donc √† jour les estimations de la valeur d'action en fonction d'une politique diff√©rente de celle utilis√©e pour ex√©cuter l'exp√©rience.\n",
    "\n",
    "Ceci est illustr√© dans l'algorithme de pseudo-code suivant :\n",
    "\n",
    "![qlearning](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/LEAD/Reinforcement-Learning/qlearning.png)\n",
    "\n",
    "Le Q-learning est g√©n√©ralement ce que nous allons utiliser dans la pratique pour un certain nombre de raisons :\n",
    "\n",
    "- Nous n'avons pas besoin d'un mod√®le de l'environnement\n",
    "- Nous n'avons pas besoin que la politique utilis√©e dans l'exp√©rience converge vers la politique optimale pour obtenir la fonction de valeur d'action optimale\n",
    "- Nous n'avons pas besoin d'attendre la fin d'un √©pisode complet pour mettre √† jour les fonctions de valeur\n",
    "\n",
    "Bien s√ªr, il y a quelques inconv√©nients au Q-learning, le principal √©tant que les rendements estim√©s peuvent √™tre largement surestim√©s (car nous utilisons les m√™mes valeurs pour s√©lectionner et √©valuer les actions, ce qui conduit l'algorithme √† s√©lectionner les actions les mieux not√©es), mais certaines techniques existent pour compenser cela comme le double Q-learning, qui tente de d√©coupler l'√©valuation de la s√©lection d'action afin de r√©duire ce risque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>R√©sum√©\n",
    " </strong></span></h2>\n",
    "\n",
    "Nous avons couvert pas mal de th√©orie, et il est temps de faire un petit r√©sum√© de tout ce que nous avons appris jusqu'√† pr√©sent :\n",
    "\n",
    "L'apprentissage par renforcement se concentre sur l'optimisation d'une certaine valeur d'objectif pour un agent interagissant avec un environnement .\n",
    "\n",
    "De nombreuses situations correspondent √† cette d√©finition, et diverses techniques aident dans diff√©rents cas :\n",
    "\n",
    "- L'environnement est compos√© d'un seul √©tat avec des r√©compenses inconnues : probl√®me de bandit\n",
    "- L'environnement peut √™tre compos√© de plusieurs √©tats, dont on a une parfaite connaissance (on conna√Æt toutes les probabilit√©s de transition d'√©tat en fonction des actions et des r√©compenses correspondantes)\n",
    "- Nous n'avons peut-√™tre aucune connaissance de l'environnement, ce qui signifie que nous ne savons pas comment nos actions peuvent ou non influencer l'√©tat de l'environnement\n",
    "\n",
    "Dans toutes ces diff√©rentes situations, nous nous concentrons sur certaines valeurs et fonctions cl√©s qui d√©crivent le comportement et les objectifs de l'agent :\n",
    "\n",
    "- La **politique** est la fa√ßon dont l'agent choisit ses actions en fonction de l' √©tat actuel de l'environnement\n",
    "- La **fonction de valeur** d'√©tat donne le retour attendu de l'agent suivant une certaine politique connaissant un certain √©tat\n",
    "- La **fonction de valeur d'√©tat-action** donne le retour attendu de l'agent suivant une certaine politique lors du choix d'une certaine action compte tenu de l'√©tat de l'environnement\n",
    "Tout au long du cours, nous nous sommes concentr√©s sur les processus de d√©cision de Markov finis, qui sont des exp√©riences al√©atoires avec des √©tats cons√©cutifs de l'environnement qui obligent l'agent √† choisir des actions afin de passer d'un √©tat √† l'autre, avec des espaces d'action et d'√©tat finis (qui peuvent cependant √™tre √©norme, pensez go game). Nous avons ensuite √©tudi√© diff√©rentes approches afin d'√©valuer les politiques dans diff√©rents cas, et de trouver des politiques optimales gr√¢ce √† des algorithmes d'am√©lioration/contr√¥le.\n",
    "\n",
    "- **Programmation dynamique** : qui demande une parfaite connaissance de l'environnement (les probabilit√©s d'√©tat-transition)\n",
    "- **M√©thodes de Monte Carlo** : qui nous permettent d'apprendre de l'exp√©rience, mais demandent la r√©alisation de chaque exp√©rience afin d'avoir des retours r√©els au travail avec\n",
    "- **Apprentissage par diff√©rence temporelle** : qui sont les plus couramment utilis√©s dans la pratique car ils nous permettent d'apprendre de l'exp√©rience sans avoir √† attendre que les rendements r√©els soient calcul√©s, mais peuvent fonctionner directement √† partir d'estimations de fonctions de valeur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ressources üìöüìö\n",
    "\n",
    "* [The reference book for all things reinforcement learning (from which the figures were taken)](https://full-stack-assets.s3.eu-west-3.amazonaws.com/references/reinforcement_learning/SuttonBartoIPRLBook2ndEd.pdf)\n",
    "* [The Deepmind (alpha go creators) reinforcement learning lectures](https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021)\n",
    "* [Easy to follow didactic videos on reinforcement learning](https://www.youtube.com/watch?v=nyjbcRQ-uQ8&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
