{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://uploads-ssl.webflow.com/62233c592d2a1e009d42f46c/6414802c0a2bea367cbc795b_logo-jedha-square.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: left; color:#20a08d; font-size: 40px\"><span><strong>Comprendre l'apprentissage par renforcement profond üß†üß†\n",
    "</strong></span></h1>\n",
    "\n",
    "Le Deep reinforcement learning consiste √† m√©langer l'apprentissage en profondeur avec des techniques classiques d'apprentissage par renforcement. L'aspect cl√© est que nous sommes confront√©s √† un environnement avec un nombre infini d'√©tats (parfois m√™me pas √©num√©rables), donc l'id√©e est de prendre la description de l'environnement comme entr√©e d'un mod√®le d'apprentissage en profondeur (il pourrait √™tre exprim√© sous forme d'images par exemple ) et le faire correspondre √† un espace de repr√©sentation que nous utiliserons comme projection de l'environnement pour prendre des d√©cisions.\n",
    "\n",
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>Qu'allez-vous apprendre dans ce cours ? üßêüßê\n",
    "</strong></span></h2>\n",
    "\n",
    "- Pr√©diction (√©valuation des politiques)\n",
    "- Contr√¥le (trouver la meilleure strat√©gie en estimant les valeurs des paires d'action d'√©tat)\n",
    "- L'apprentissage profond (Deep Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif du mod√®le est de cartographier des √©tats de l'environnement qui sont similaires √† des repr√©sentations connues de l'agent afin que l'agent puisse prendre plus simplement les d√©cisions qui lui rapporteront le maximum de r√©compense cumul√©e.\n",
    "\n",
    "Nous utiliserons **l'apprentissage par renforcement profond dans des situations o√π l'espace d'√©tat n'est pas fini**. Des exemples pourraient √™tre que nous avons des m√©triques avec des espaces de valeurs continus, comme l'environnement cartpole. Ou vous pourriez penser √† un jeu d'arcade comme atari's pong, o√π nous essayons de comprendre quelles actions entreprendre en fonction des images produites par le jeu.\n",
    "\n",
    "Comme nous avons beaucoup d'√©tats (un nombre infini), le Q-learning ne peut pas fonctionner, car le Q-learning associe une valeur √† chaque paire √©tat-action. Nous allons donc devoir proc√©der √† l'approximation de la fonction. Cela signifie utiliser une fonction pour mapper les √©tats et les actions sur des valeurs, et utiliser cette fonction pour extrapoler la valeur des √©tats observ√©s afin d'estimer les valeurs des √©tats ressemblants non observ√©s.\n",
    "\n",
    "L'apprentissage par renforcement profond **utilise des r√©seaux de neurones** pour mapper les √©tats √† un espace de repr√©sentation sp√©cifique qui tente de produire les valeurs de chaque paire d'action d'√©tat.\n",
    "\n",
    "Explorons les m√©thodes de pr√©diction et de contr√¥le, c'est-√†-dire l'estimation des valeurs d'√©tat et l'estimation des valeurs de paires action-√©tat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>Prediction üîÆ</strong></span></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximation de la fonction de valeur (√©tat -> r√©compense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S'il y a beaucoup d'√©tats, nous avons besoin de beaucoup d'exp√©rience pour apprendre toutes les valeurs d'√©tats , nous utilisons donc l'approximation de la fonction afin de g√©n√©raliser la valeur des √©tats observ√©s √† des √©tats similaires.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "v_w (s) & \\approx v_{\\pi}(s) \\\\\n",
    "q_w(s,a) & \\approx q_{\\pi}(s,a)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Notre objectif est de mettre √† jour le jeu de param√®tres $w$ et obtenez une fonction qui peut se g√©n√©raliser √† des √©tats non encore vus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>Approximation de la fonction lin√©aire\n",
    "</strong></span></h3>\n",
    "\n",
    "Mappez les √©tats sur un ensemble de nombres r√©els comme (position de la base, vitesse de la base, angle du bras, vitesse angulaire du bras)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x :& \\mathcal{S} \\rightarrow \\mathbb{R}^n \\\\\n",
    "& s \\rightarrow x(s)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Ensuite, nous faisons une approximation lin√©aire des fonctions de valeur :\n",
    "\n",
    "$$\n",
    "v_w(s) = w^Tx(s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>Coarse Coding (codage grossier)\n",
    "</strong></span></h3>\n",
    "\n",
    "Si nous souhaitons mapper notre espace d'√©tats infini sur un espace d'√©tats fini, nous pouvons recourir √† une technique appel√©e **coarse coding** (ou alternativement **tile coding**). Cette technique permet de discr√©tiser l'espace en le mappant sur une collection de variables binaires dont les valeurs correspondent √† :\n",
    "\n",
    "- 0 : l'√©tat n'appartient pas √† cet espace de valeurs\n",
    "- 1 : l'√©tat appartient √† cet espace de valeurs\n",
    "\n",
    "Cela nous permet d'utiliser les valeurs obtenues √† partir des √©tats observ√©s pour influencer les valeurs des √©tats voisins. La pr√©cision que vous recherchez et la puissance de calcul dont vous disposez influenceront fortement le type de cartographie que vous ferez.\n",
    "\n",
    "Voici un exemple de codage grossier pour un espace d'√©tat √† deux dimensions :\n",
    "![coarse coding](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/LEAD/Reinforcement-Learning/coarse_coding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>Approximation de la fonction diff√©rentiable\n",
    "\n",
    "</strong></span></h3>\n",
    "\n",
    "$v_w(s)$ est une fonction diff√©rentiable avec un ensemble de param√®tres $w$ qui pourrait √™tre non lin√©aire\n",
    "\n",
    "Vous pouvez √©galement imaginer un cas o√π les les \"features maps\" que la fonction de valeur obtient en tant qu'entr√©es sont apprises et non corrig√©es (comme si vous essayez d'interpr√©ter une image d'un jeu comme un vecteur de caract√©ristiques ou quelque chose)\n",
    "\n",
    "Consid√©rons une fonction $J(w)$, qui jouerait le r√¥le de la fonction \"perte\" pour notre probl√®me d'apprentissage par renforcement.\n",
    "\n",
    "Le gradient de $J$ est\n",
    "\n",
    "$$\n",
    "\\nabla_w J(w) = (\\frac{\\partial J(w)}{\\partial w_1}, \\dots, \\frac{\\partial J(w)}{\\partial w_p})\n",
    "$$\n",
    "\n",
    "Notre objectif est de minimiser J. Nous pourrions faire une descente de gradient et nous d√©placer dans le sens oppos√© du gradient\n",
    "\n",
    "$$\n",
    "w^{(t+1)}=w^{(t)} - \\frac{1}{2}\\gamma \\nabla_w J(w)\n",
    "$$\n",
    "\n",
    "si\n",
    "\n",
    "$$\n",
    "J(w) = \\mathbb{E}_{S~d}[(v_{\\pi}(S) - v_w(S))^2]\n",
    "$$\n",
    "\n",
    "L'erreur quadratique attendue entre la politique et sa politique bas√©e sur l'approximation de la fonction de valeur\n",
    "\n",
    "La descente du gradient ressemble alors √†\n",
    "$$\n",
    "w^{(t+1)}=w^{(t)} - \\gamma \\mathbb{E}_{S~d}[v_{\\pi}(S) - v_w(S)] \\nabla_w v_w(S)\n",
    "$$\n",
    "\n",
    "√âtant donn√© qu'en pratique, la valeur attendue r√©elle peut √™tre inobservable ou intraitable, nous pouvons alors choisir d'utiliser la descente de gradient stochastique en travaillant avec des estimations \n",
    "\n",
    "$$\n",
    "w^{(t+1)}=w^{(t)} - \\gamma (G_t - v_w(S_t)) \\nabla_w v_w(S_t)\n",
    "$$\n",
    "\n",
    "O√π nous mettons progressivement √† jour le $G_t$ gr√¢ce √† une m√©thode de Monte Carlo (o√π $G_t$ est la r√©compense cumul√©e apr√®s observation de l'√©tat $S_t$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>Algorithme de contr√¥le üéÆ</strong></span></h2>\n",
    "\n",
    "Comme toujours, l'√©valuation des valeurs des √©tats ne repr√©sente que la moiti√© du probl√®me d'apprentissage par renforcement. Afin de le r√©soudre, nous devons savoir quelles actions conduisent aux niveaux de r√©compense optimaux\n",
    "\n",
    "- Le Q-learning peut √™tre √©tendu √† l'approximation de fonctions (r√©seau Q profond - DQN)\n",
    "- Th√©orie pas compl√®tement d√©velopp√©e\n",
    "- Le \"tracking\" (ce qui signifie que la politique peut s'adapter en permanence) est g√©n√©ralement pr√©f√©r√© √† la convergence vers une politique fixe\n",
    "\n",
    "Pour la descente de gradient par lots avec l'apprentissage par renforcement :\n",
    "\n",
    "1. Le r√©seau de neurones prend l'observation O pour la transformer en une valeur d'action pour chaque action possible, puis nous utilisons une politique epsilon-greedy pour prendre l'action suivante.\n",
    "\n",
    "2. Ensuite, les mises √† jour de poids se d√©roulent comme suit :\n",
    "$$\n",
    "\\Delta w \\propto (R_{t+1} + \\gamma \\max_a q_w (S_{t+1},a) - q_w (S_t, A_t)) \\nabla_w q_w (S_t, A_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span><strong>Algorithme DQN\n",
    "</strong></span></h3>\n",
    "\n",
    "1. $O_t \\rightarrow q_w$ : Mettre plusieurs \"frames\" ensemble pour les jeux atari par exemple, donc une observation n'est pas n√©cessairement un seul √©tat mais une s√©quence de plusieurs √©tats.\n",
    "\n",
    "2. Nous utilisons une politique d'exploration (epsilon-greedy)\n",
    "\n",
    "3. Nous gardons √©galement un tampon de relecture pour stocker des exemples de transitions pass√©es $(S_i, A_i, R_{i+1},S_{i+1})$ Celles-ci seront utilis√©es pour entra√Æner le mod√®le pendant plusieurs it√©rations jusqu'√† ce qu'elles soient remplac√©es par de nouvelles observations au fil du temps.\n",
    "\n",
    "4. Nous avons mis en place des param√®tres de r√©seau cible $w^{-}$ qui reste constant sur plusieurs pas de temps. Le r√©seau cible est utilis√© pour s√©lectionner la meilleure action dans l'√©tat suivant de l'environnement (cela emp√™che l'apprentissage Q pour s√©lectionner des actions qui suivent strictement l'√©volution des estimations, et √©vite de surestimer certaines paires de valeurs d'action d'√©tat).\n",
    "\n",
    "5. La mise √† jour des poids Q learning sur $w$ (utilise les donn√©es de relecture et le r√©seau cible)\n",
    "\n",
    "$$\n",
    "\\Delta w \\propto (R_{t+1} + \\gamma \\max_a q_{w^{-}} (S_{i+1},a) - q_w (S_i, A_i)) \\nabla_w q_w (S_i, A_i)\n",
    "$$\n",
    "\n",
    "6. Ensuite, nous mettons √† jour les param√®tres du r√©seau cible$w_t^{-} \\leftarrow w_t$ occasionnellement (tous les 10 000 pas par exemple)\n",
    "\n",
    "L'utilisation de donn√©es de relecture et d'un r√©seau cible fait ressembler DQN √† un apprentissage supervis√©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>Quel r√¥le jouent les r√©seaux de neurones profonds dans tout cela ?üìà</strong></span></h2>\n",
    "\n",
    "L'apprentissage en profondeur entre en jeu pour la partie d'approximation de la fonction, que vous traitiez un probl√®me de pr√©diction (estimer la fonction de valeur pour chaque √©tat donn√© une certaine politique) ou le probl√®me de contr√¥le (estimer la fonction de valeur de l'√©tat d'action pour chaque √©tat et action ) le r√©seau neuronal profond aidera √† mapper les √©tats sur des valeurs sp√©cifiques ou des paires de valeurs d'action.\n",
    "\n",
    "Par exemple:\n",
    "- Dans le cas de la pr√©diction (estimation de la fonction de valeur pour chaque √©tat), le mod√®le de r√©seau neuronal profond sera une fonction prenant ses entr√©es dans $\\mathbb{R}^n$ $n$ √©tant la dimensionnalit√© de l'espace d'√©tat (par exemple, si l'espace d'√©tat est l'√©cran d'un jeu atari avec une r√©solution de 200x200 en couleurs, alors la dimension de l'entr√©e sera $(200,200,3)$, et produisant des sorties dans $\\mathbb{R}$ pour les valeurs. Vous pouvez consid√©rer cela comme un r√©seau neuronal convolutif avec une derni√®re couche contenant un neurone avec une fonction d'activation lin√©aire.\n",
    "- Dans le cas du contr√¥le (estimation des paires de valeurs d'√©tat d'action), le r√©seau de neurones a des entr√©es dans $\\mathbb{R}^n$, et sorties en $\\mathbb{R}^m$ , o√π $n$ est la dimensionnalit√© de l'espace d'entr√©e, et $m$ le nombre d'actions possibles. Pour l'exemple du jeu atari, il peut s'agir d'un r√©seau de neurones convolutifs avec une derni√®re couche dense avec $m$ neurones et activation lin√©aire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span><strong>Deadly triad üíÄ</strong></span></h2>\n",
    "\n",
    "La Deadly triad (triade mortelle) est une combinaison de \"raccourcis\" utilis√©s dans l'apprentissage par renforcement pour acc√©l√©rer ou simplifier le processus d'entrainement qui peut emp√™cher le mod√®le de converger du tout vers la politique optimale.\n",
    "\n",
    "Ces trois √©l√©ments sont :\n",
    "\n",
    "- **bootstraping (apprentissage √† partir d'estimations)** : qui est l'aspect central du Q-learning\n",
    "- **approximation de fonction** : qui est au c≈ìur de l'apprentissage par renforcement profond, puisque nous avons affaire √† des espaces d'√©tats observables qui sont infinis\n",
    "- **apprentissage hors politique** : qui est au c≈ìur du Q-learning, puisque nous utilisons une politique epsilon-greedy afin d'apprendre la politique optimale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ressources üìöüìö\n",
    "\n",
    "* [The reference book for all things reinforcement learning (from which the figures were taken)](https://full-stack-assets.s3.eu-west-3.amazonaws.com/references/reinforcement_learning/SuttonBartoIPRLBook2ndEd.pdf)\n",
    "* [The Deepmind (alpha go creators) reinforcement learning lectures](https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
