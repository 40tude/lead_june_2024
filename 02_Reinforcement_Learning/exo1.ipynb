{"cells":[{"cell_type":"markdown","metadata":{"id":"pYZoHy6_TDQG"},"source":["# Q Learning on Frozen Lake\n","\n","This exercise will challenge you to solve the Reinforcement Learning problem in the Frozen Lake environment, it seems quite simple but watch out! It may have a few suprises in store!\n","\n","Frozen lake involves crossing a frozen lake from Start(S) to Goal(G) without falling into any Holes(H) by walking over the Frozen(F) lake.\n","\n","## Action Space\n","The agent takes a 1-element vector for actions. The action space is (dir), where dir decides direction to move in which can be:\n","\n","0: LEFT\n","\n","1: DOWN\n","\n","2: RIGHT\n","\n","3: UP\n","\n","## Observation Space\n","The observation is a value representing the agentâ€™s current position as current_row * nrows + current_col (where both the row and col start at 0). For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15. The number of possible observations is dependent on the size of the map. For example, the 4x4 map has 16 possible observations.\n","\n","## Rewards\n","Reward schedule:\n","\n","Reach goal(G): +1\n","\n","Reach hole(H): 0 (Terminates the episode)\n","\n","Reach frozen(F): 0"]},{"cell_type":"markdown","metadata":{"id":"8hlzu6RJTDQH"},"source":["1. Let's start by installing some libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14549,"status":"ok","timestamp":1719841386024,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"6xCjKhE4TDQH","outputId":"4505e206-49cf-4ffd-c2aa-2873cf34fdcd"},"outputs":[],"source":["!pip install cmake\n","!pip install scipy\n","!pip install gymnasium"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DNP_SiR0kcg8"},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","import random\n","from IPython.display import clear_output"]},{"cell_type":"markdown","metadata":{"id":"BcQzVJ5nTDQI"},"source":["2. Use `gym` to set up a frozen Lake environment with dimension 4x4 that is not slippery. You may learn how to do that [here](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/). Reset the environment and display the first state using matplotlib and the `.render(mode=\"rgb_array\")` method."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":314},"executionInfo":{"elapsed":903,"status":"ok","timestamp":1719841410729,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"ahoxis_NTDQI","outputId":"634065a7-1267-4050-9ef4-1a81283cf019"},"outputs":[],"source":["print(\"[INFO] : Version Gym : \", gym.__version__)\n","\n","env = gym.make(\"FrozenLake-v1\", desc=None, map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\").env\n","env.reset()\n","env.render()"]},{"cell_type":"markdown","metadata":{"id":"Iqx7xpMHTDQI"},"source":["3. Reset the environment and look what happens when taking action `0`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":425,"status":"ok","timestamp":1719841416958,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"xEFCVdfzTDQI","outputId":"2f096431-afb3-4e4e-d068-3c748fe7c0f2"},"outputs":[],"source":["env.reset()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":380,"status":"ok","timestamp":1719841419399,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"0flF1r-XTDQI","outputId":"32ccf0c7-335f-44b0-e0b1-0461fabf1d93"},"outputs":[],"source":["env.step(0)\n"]},{"cell_type":"markdown","metadata":{"id":"xb2FlHgOTDQI"},"source":["4. How do you interpret the resulting values?"]},{"cell_type":"markdown","metadata":{"id":"FyU-Ri0STDQI"},"source":["5. Print the size of the action space and the observation space using attributes of the environment object."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":407,"status":"ok","timestamp":1719841423394,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"NE4P0JYJTDQI","outputId":"a9ad55e2-4b71-4f74-fa97-436abe071dd4"},"outputs":[],"source":["print(\"Action Space {}\".format(env.action_space))\n","print(\"State Space {}\".format(env.observation_space))\n"]},{"cell_type":"markdown","metadata":{"id":"9cFR_qQJTDQJ"},"source":["6. Setup the Q Table (remember it should represent the values of each state action pairs), initialize all values to zero"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":367,"status":"ok","timestamp":1719841429024,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"YcMS5BqRTDQJ","outputId":"44dbf888-b196-4896-90e9-7d33f00588d9"},"outputs":[],"source":["q_table = np.zeros([env.observation_space.n, env.action_space.n])\n","q_table"]},{"cell_type":"markdown","metadata":{"id":"O_bcuJRsTDQJ"},"source":["7. Run a Q-Learning Loop inspired from the demo over 10 000 episodes."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36442,"status":"ok","timestamp":1719841600893,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"_Kze0RAsTDQJ","outputId":"53c8df24-da2b-423b-9263-3cb46b9c6e46"},"outputs":[],"source":["%%time\n","# magic command for measuring the cell's exectution time\n","\"\"\"Training the agent\"\"\"\n","\n","# Hyperparameters\n","alpha = 0.1\n","gamma = 0.6\n","epsilon = 0.1\n","\n","# For plotting metrics\n","all_epochs = []\n","all_penalties = []\n","\n","# Loop over a certain number off episodes\n","for i in range(1, 10001):\n","    state, info = env.reset() # start by re-initializing the environment\n","\n","    epochs, penalties, reward, = 0, 0, 0\n","    done = False\n","\n","    while not done: # starting a while loop that will keep running until the termination of an episode\n","        # We then define the epsilon greedy policy\n","        if random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample() # Explore action space\n","        else:\n","            action = np.argmax(q_table[state]) # Exploit learned values\n","\n","        # take action and get next state information\n","        next_state, reward, done, _, info = env.step(action)\n","\n","        # update q table using the algorithm formula\n","        old_value = q_table[state, action]\n","        next_max = np.max(q_table[next_state])\n","\n","        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max) # decaying average of the old value and the new estimated value\n","        q_table[state, action] = new_value\n","\n","        # update the state variable\n","        state = next_state\n","        epochs += 1\n","\n","    # every 100 episode we print the episode number\n","    if i % 100 == 0:\n","        clear_output(wait=True)\n","        print(f\"Episode: {i}\")\n","\n","print(\"Training finished.\\n\")"]},{"cell_type":"markdown","metadata":{"id":"Ib_l5EgRTDQJ"},"source":["8. Try and visualize what the agent is doing using `.render`, does anything surprise you? Why do you think this is happening?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":7231,"status":"ok","timestamp":1719841651456,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"tgeK9WgcTDQJ","outputId":"49fe826f-f6b5-498c-9212-7cd3975b1349"},"outputs":[],"source":["# watch trained agent\n","state, info = env.reset()\n","done = False\n","rewards = 0\n","max_steps = 20\n","\n","for s in range(max_steps):\n","\n","    print(f\"TRAINED AGENT\")\n","    print(\"Step {}\".format(s+1))\n","\n","    action = np.argmax(q_table[state])\n","    new_state, reward, done, _, info = env.step(action)\n","    rewards += reward\n","    plt.imshow(env.render())\n","    plt.show()\n","    print(f\"score: {rewards}\")\n","    state = new_state\n","\n","    if done == True:\n","        break\n","\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"n9rZB8EwTDQJ"},"source":["9. The agent does not move! Although we know for a fact this is not the optimal behaviour! The problem comes from the fact that we start with a Q-table filled with zeros. That means that at the beginning, the optimal action given by `np.argmax(q_table[state])` is always `0` which lead our agent against the wall. The odds of picking random actions are really low with our chosen policy (e.g. 0.1). In this setting, it becomes almost impossible for the agent to randomly reach the goal (the only non zero reward) and start learning!\n","\n","Try to think of a solution for this for at least 5 minutes then click the spoiler to get a clue:\n","\n","<details>\n","<summary>SPOILER</summary>\n","The solution is to force the agent to pick a random action whenever the score for all actions are equivalent\n","</details>\n","\n","Once you think you have found a way to solve this issue, rerun your adapted training loop."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4811,"status":"ok","timestamp":1719841765116,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"TKCd1XuwTDQJ","outputId":"f0d99c0d-9ade-4fa6-8e78-aaea4acacd4c"},"outputs":[],"source":["%%time\n","# magic command for measuring the cell's exectution time\n","\"\"\"Training the agent\"\"\"\n","\n","# Hyperparameters\n","alpha = 0.1\n","gamma = 0.6\n","epsilon = 0.1\n","\n","# For plotting metrics\n","all_epochs = []\n","all_penalties = []\n","\n","# Loop over a certain number off episodes\n","for i in range(1, 10001):\n","    state, info = env.reset() # start by re-initializing the environment\n","\n","    epochs, penalties, reward, = 0, 0, 0\n","    done = False\n","\n","    while not done: # starting a while loop that will keep running until the termination of an episode\n","        # We then define the epsilon greedy policy\n","        if random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample() # Explore action space\n","        elif np.min(q_table[state]) == np.max(q_table[state]):\n","            action = env.action_space.sample() # Explore action space\n","        else:\n","            action = np.argmax(q_table[state]) # Exploit learned values\n","\n","        # take action and get next state information\n","        next_state, reward, done, _, info = env.step(action)\n","\n","        # update q table using the algorithm formula\n","        old_value = q_table[state, action]\n","        next_max = np.max(q_table[next_state])\n","\n","        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max) # decaying average of the old value and the new estimated value\n","        q_table[state, action] = new_value\n","\n","        # update the state variable\n","        state = next_state\n","        epochs += 1\n","\n","    # every 100 episode we print the episode number\n","    if i % 100 == 0:\n","        clear_output(wait=True)\n","        print(f\"Episode: {i}\")\n","\n","print(\"Training finished.\\n\")"]},{"cell_type":"markdown","metadata":{"id":"wmtHveHrTDQJ"},"source":["10. Calculate the average reward across 100 episodes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":407,"status":"ok","timestamp":1719841800548,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"pBu3oLEITDQJ","outputId":"31679859-5d43-489c-f0cf-e64c36b5fabd"},"outputs":[],"source":["\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n","\n","total_epochs, total_penalties = 0, 0\n","episodes = 100\n","cumulated_reward = 0\n","\n","for _ in range(episodes):\n","    state, info = env.reset()\n","    epochs, penalties, reward = 0, 0, 0\n","\n","    done = False\n","\n","    while not done:\n","        # this time we use the greedy policy\n","        action = np.argmax(q_table[state])\n","        state, reward, done, _, info = env.step(action)\n","\n","        cumulated_reward += reward\n","\n","        epochs += 1\n","\n","    total_epochs += epochs\n","\n","print(f\"Results after {episodes} episodes:\")\n","print(f\"Average reward per episode: {cumulated_reward / episodes}\")"]},{"cell_type":"markdown","metadata":{"id":"jANzNtxrTDQJ"},"source":["11. What is the agent doing now? Show its behaviour visually."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2462,"status":"ok","timestamp":1719841818673,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"Sd0JwJOATDQJ","outputId":"c54dc8a9-a628-4d63-a5ed-664a0f21c31e"},"outputs":[],"source":["# watch trained agent\n","state, info = env.reset()\n","done = False\n","rewards = 0\n","max_steps = 100\n","\n","for s in range(max_steps):\n","\n","    print(f\"TRAINED AGENT\")\n","    print(\"Step {}\".format(s+1))\n","\n","    action = np.argmax(q_table[state])\n","    new_state, reward, done, _, info = env.step(action)\n","    rewards += reward\n","    plt.imshow(env.render())\n","    plt.show()\n","    print(f\"score: {rewards}\")\n","    state = new_state\n","\n","    if done == True:\n","        break\n","\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"qzEkYnoLTDQJ"},"source":["Yay! The agent is now able to win the game in an optimal way!"]},{"cell_type":"markdown","metadata":{"id":"_9icJ-U7TDQJ"},"source":["## Frozen Lake 8x8\n","\n","No let's see if we can solve the frozen lake problem with a more challenging map!\n","\n","1. Setup a Frozen Lake environment with an 8x8 map and not slippery."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":570},"executionInfo":{"elapsed":430,"status":"ok","timestamp":1719841920175,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"XVPUrMiQTDQJ","outputId":"e171e3b8-eea3-4623-c7dd-e8af71a8a812"},"outputs":[],"source":["print(\"[INFO] : Version Gym : \", gym.__version__)\n","\n","env = gym.make(\"FrozenLake-v1\", desc=None,map_name=\"8x8\", is_slippery=False, render_mode=\"rgb_array\").env\n","env.reset()\n","env.render()"]},{"cell_type":"markdown","metadata":{"id":"-wB8j2zvTDQJ"},"source":["2. In this setting, the probability to randomly reach the objective is way thinner, let's see if our training loop has any chance to complete the Q-learning algorithm. Setup the Q table with initial values equal to zero."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":362,"status":"ok","timestamp":1719842010271,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"LAkJPc4lTDQJ","outputId":"8c91e278-75ed-4abf-a62f-440fb6edccdf"},"outputs":[],"source":["q_table = np.zeros([env.observation_space.n, env.action_space.n])\n","q_table"]},{"cell_type":"markdown","metadata":{"id":"h5Pjb7QPTDQJ"},"source":["3. Run the Q-learning algorithm over 100 000 episodes."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":81720,"status":"ok","timestamp":1719842257265,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"XJRO2KOkTDQK","outputId":"20d20e59-36ea-4b35-ec8a-8fd06ec76f3a"},"outputs":[],"source":["%%time\n","\"\"\"Training the agent\"\"\"\n","\n","\n","# Hyperparameters\n","alpha = 0.1\n","gamma = 0.6\n","epsilon = 0.1\n","\n","# For plotting metrics\n","all_epochs = []\n","all_penalties = []\n","\n","# Loop over a certain number off episodes\n","for i in range(1, 100001):\n","    state, info = env.reset() # start by re-initializing the environment\n","\n","    epochs, penalties, reward, = 0, 0, 0\n","    done = False\n","\n","    while not done: # starting a while loop that will keep running until the termination of an episode\n","        # We then define the epsilon greedy policy\n","        if random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample() # Explore action space\n","        elif np.min(q_table[state]) == np.max(q_table[state]):\n","            action = env.action_space.sample() # Explore action space\n","        else:\n","            action = np.argmax(q_table[state]) # Exploit learned values\n","\n","        # take action and get next state information\n","        next_state, reward, done, _, info = env.step(action)\n","\n","        # update q table using the algorithm formula\n","        old_value = q_table[state, action]\n","        next_max = np.max(q_table[next_state])\n","\n","        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max) # decaying average of the old value and the new estimated value\n","        q_table[state, action] = new_value\n","\n","        # update the state variable\n","        state = next_state\n","        epochs += 1\n","\n","    # every 100 episode we print the episode number\n","    if i % 100 == 0:\n","        clear_output(wait=True)\n","        print(f\"Episode: {i}\")\n","\n","print(\"Training finished.\\n\")"]},{"cell_type":"markdown","metadata":{"id":"g0ykc5QVTDQK"},"source":["4. Visualize the agent's behaviour."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ADobEu4FAjxRpZICng2kr0BczXvDAZIK"},"executionInfo":{"elapsed":6965,"status":"ok","timestamp":1719842270700,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"OEkafPJCTDQK","outputId":"e0e73be6-a7a5-4cd4-da29-8542099a377d"},"outputs":[],"source":["state, info = env.reset()\n","done = False\n","rewards = 0\n","max_steps = 100\n","\n","for s in range(max_steps):\n","\n","    print(f\"TRAINED AGENT\")\n","    print(\"Step {}\".format(s+1))\n","\n","    action = np.argmax(q_table[state])\n","    new_state, reward, done, _, info = env.step(action)\n","    rewards += reward\n","    plt.imshow(env.render())\n","    plt.show()\n","    print(f\"score: {rewards}\")\n","    state = new_state\n","\n","    if done == True:\n","        break\n","\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"rK-ReJ_QTDQK"},"source":["Looks like it reached the goal!"]},{"cell_type":"markdown","metadata":{"id":"IzoKGxkKTDQK"},"source":["## Slippery Frozen Lake\n","\n","Now let's complicate things even further by making the lake slippery! This means that whenever you pick an action you have two out of three chances to go lateral instead of going forward (also with one third of a chance). For example if I pick the action \"down\", the probability of going \"down\" is $\\frac{1}{3}$ the probability of going \"left\" is $\\frac{1}{3}$ and the probability of going \"right\" is $\\frac{1}{3}$."]},{"cell_type":"markdown","metadata":{"id":"Ldhsov3xTDQK"},"source":["1. Setup an environment with map 8x8 in slippery mode."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":570},"executionInfo":{"elapsed":436,"status":"ok","timestamp":1719842363173,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"rKeLOIjmTDQK","outputId":"f29fd9f6-1e0b-4832-a0b9-4b9ce9764249"},"outputs":[],"source":["print(\"[INFO] : Version Gym : \", gym.__version__)\n","\n","\n","env = gym.make(\"FrozenLake-v1\", desc=None,map_name=\"8x8\", is_slippery=False, render_mode=\"rgb_array\").env\n","env.reset()\n","env.render()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oEMsyiIXTDQK"},"source":["2. Setup the Q table"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":324,"status":"ok","timestamp":1719842379627,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"ruO-ObSVTDQK","outputId":"29a002b5-7b07-432a-ce24-4f37174d5be6"},"outputs":[],"source":["q_table = np.zeros([env.observation_space.n, env.action_space.n])\n","q_table"]},{"cell_type":"markdown","metadata":{"id":"Zj00hELzTDQK"},"source":["3. Run the Q learning algorithm over 100 000 episodes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":83563,"status":"ok","timestamp":1719842599713,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"xRopCIK-TDQK","outputId":"e9157358-8e47-4d81-af3e-409ec6729753"},"outputs":[],"source":["%%time\n","# magic command for measuring the cell's exectution time\n","\"\"\"Training the agent\"\"\"\n","\n","# Hyperparameters\n","alpha = 0.1\n","gamma = 0.6\n","epsilon = 0.1\n","\n","# For plotting metrics\n","all_epochs = []\n","all_penalties = []\n","\n","# Loop over a certain number off episodes\n","for i in range(1, 100001):\n","    state, info = env.reset() # start by re-initializing the environment\n","\n","    epochs, penalties, reward, = 0, 0, 0\n","    done = False\n","\n","    while not done: # starting a while loop that will keep running until the termination of an episode\n","        # We then define the epsilon greedy policy\n","        if random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample() # Explore action space\n","        elif np.min(q_table[state]) == np.max(q_table[state]):\n","            action = env.action_space.sample() # Explore action space\n","        else:\n","            action = np.argmax(q_table[state]) # Exploit learned values\n","\n","        # take action and get next state information\n","        next_state, reward, done, _, info = env.step(action)\n","\n","        # update q table using the algorithm formula\n","        old_value = q_table[state, action]\n","        next_max = np.max(q_table[next_state])\n","\n","        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max) # decaying average of the old value and the new estimated value\n","        q_table[state, action] = new_value\n","\n","        # update the state variable\n","        state = next_state\n","        epochs += 1\n","\n","    # every 100 episode we print the episode number\n","    if i % 100 == 0:\n","        clear_output(wait=True)\n","        print(f\"Episode: {i}\")\n","\n","print(\"Training finished.\\n\")"]},{"cell_type":"markdown","metadata":{"id":"wegcIteMTDQL"},"source":["4. Visualize what the agent is doing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1_YAGHQhUNu1xe8iHLwU7bpQEBSenRhEW"},"executionInfo":{"elapsed":7424,"status":"ok","timestamp":1719842615961,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"S05A2qGSTDQL","outputId":"af16d282-9840-44e4-b9f6-2c1bd8a3d4b2"},"outputs":[],"source":["# watch trained agent\n","state, info = env.reset()\n","done = False\n","rewards = 0\n","max_steps = 100\n","\n","for s in range(max_steps):\n","\n","    print(f\"TRAINED AGENT\")\n","    print(\"Step {}\".format(s+1))\n","\n","    action = np.argmax(q_table[state])\n","    new_state, reward, done, _, info = env.step(action)\n","    rewards += reward\n","    plt.imshow(env.render())\n","    plt.show()\n","    print(f\"score: {rewards}\")\n","    state = new_state\n","\n","    if done == True:\n","        break\n","\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"DOjlLePtTDQL"},"source":["5. Looks like the agent is moving, let's see what its average reward is across one hundred episodes under the greedy policy."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1623,"status":"ok","timestamp":1719842677760,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"rCH0ZIwNTDQL","outputId":"108584c5-59c6-42d4-c103-b3cb1fe417eb"},"outputs":[],"source":["\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n","\n","total_epochs, total_penalties = 0, 0\n","episodes = 100\n","cumulated_reward = 0\n","\n","for i in range(episodes):\n","    clear_output(wait=True)\n","    print(i)\n","    state, info = env.reset()\n","    epochs, penalties, reward = 0, 0, 0\n","\n","    done = False\n","\n","    while not done:\n","        # this time we use the greedy policy\n","        action = np.argmax(q_table[state])\n","        state, reward, done, _, info = env.step(action)\n","\n","        cumulated_reward += reward\n","\n","        epochs += 1\n","\n","    total_epochs += epochs\n","\n","\n","print(f\"Results after {episodes} episodes:\")\n","print(f\"Average reward per episode: {cumulated_reward / episodes}\")"]},{"cell_type":"markdown","metadata":{"id":"PiMvd6-jTDQL"},"source":["6. The agent has not learned nothing, since it reached the goal in 6% of trials. Is there a way of getting a reward of 1 100% of the time? Try to run the algorithm for 500 000 more steps to see if we improve our score!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":414317,"status":"ok","timestamp":1719843196615,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"VzXf2x7ATDQL","outputId":"0062f398-4ea4-49ba-e1ca-20eaaded4fe0"},"outputs":[],"source":["%%time\n","# magic command for measuring the cell's exectution time\n","\"\"\"Training the agent\"\"\"\n","\n","\n","# Hyperparameters\n","alpha = 0.1\n","gamma = 0.6\n","epsilon = 0.1\n","\n","# For plotting metrics\n","all_epochs = []\n","all_penalties = []\n","\n","# Loop over a certain number off episodes\n","for i in range(1, 500001):\n","    state, info = env.reset() # start by re-initializing the environment\n","\n","    epochs, penalties, reward, = 0, 0, 0\n","    done = False\n","\n","    while not done: # starting a while loop that will keep running until the termination of an episode\n","        # We then define the epsilon greedy policy\n","        if random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample() # Explore action space\n","        elif np.min(q_table[state]) == np.max(q_table[state]):\n","            action = env.action_space.sample() # Explore action space\n","        else:\n","            action = np.argmax(q_table[state]) # Exploit learned values\n","\n","        # take action and get next state information\n","        next_state, reward, done, _, info = env.step(action)\n","\n","        # update q table using the algorithm formula\n","        old_value = q_table[state, action]\n","        next_max = np.max(q_table[next_state])\n","\n","        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max) # decaying average of the old value and the new estimated value\n","        q_table[state, action] = new_value\n","\n","        # update the state variable\n","        state = next_state\n","        epochs += 1\n","\n","    # every 100 episode we print the episode number\n","    if i % 100 == 0:\n","        clear_output(wait=True)\n","        print(f\"Episode: {i}\")\n","\n","print(\"Training finished.\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7206,"status":"ok","timestamp":1719843286446,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"I-wLnY-RTDQL","outputId":"446ee2ac-d7de-4bfd-89c5-60fec2de90e9"},"outputs":[],"source":["\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n","\n","total_epochs, total_penalties = 0, 0\n","episodes = 1000\n","cumulated_reward = 0\n","\n","for i in range(episodes):\n","    clear_output(wait=True)\n","    print(i)\n","    state, info = env.reset()\n","    epochs, penalties, reward = 0, 0, 0\n","\n","    done = False\n","\n","    while not done:\n","        # this time we use the greedy policy\n","        action = np.argmax(q_table[state])\n","        state, reward, done, _, info = env.step(action)\n","\n","        cumulated_reward += reward\n","\n","        epochs += 1\n","\n","    total_epochs += epochs\n","\n","print(f\"Results after {episodes} episodes:\")\n","print(f\"Average reward per episode: {cumulated_reward / episodes}\")"]},{"cell_type":"markdown","metadata":{"id":"LEqyBGcZTDQL"},"source":["7. Looks like in this case, more training does not let us win everytime! Try and pick the actions manually to get 100% chance of winning!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":631,"status":"ok","timestamp":1719843383411,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"yzhYWyGDTDQL","outputId":"ceb5e8d7-72a0-4bfd-e2d7-542a8f7687b3"},"outputs":[],"source":["env.reset()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"94U8SllSTDQL","outputId":"0a76c2eb-c8ec-4893-86f6-e76a962f105f"},"outputs":[],"source":["done = False\n","action = 3\n","state=0\n","while done == False:\n","    if state == 7:\n","        action = 2\n","    obs = env.step(action)\n","    state, reward, done, _, info = obs\n","    print(obs)"]},{"cell_type":"markdown","metadata":{"id":"Aydr3eVqTDQL"},"source":["It's technically feasible to reach the goal with probability 100% when picking the exact right policy, and even though the Q learning algorithm should ultimately converge to the optimal policy it may be very computationally expensive to get there, even with such a simple problem!"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 ('tf': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"fbc4d3870518eee81184ced0d2279c769a0eca59aab465c4e7ec13e5e6c47a3e"}}},"nbformat":4,"nbformat_minor":0}
