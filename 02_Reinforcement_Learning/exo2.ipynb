{"cells":[{"cell_type":"markdown","id":"ddfd84ca","metadata":{"id":"ddfd84ca"},"source":["# Q learning on Cart Pole\n","\n","This exercise will focus on solving the reinforcement learning problem for the Cart Pole environment.\n","\n","## Description\n","This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in “Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem”. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n","\n","## Action Space\n","The action is a ndarray with shape (1,) which can take values {0, 1} indicating the direction of the fixed force the cart is pushed with.\n","\n","Action:\n","* 0: Push cart to the left\n","* 1: Push cart to the right\n","\n","Note: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n","\n","## Observation Space\n","The observation is a ndarray with shape (4,) with the values corresponding to the following positions and velocities:\n","\n","Num\n","\n","Observation\n","\n","Min\n","\n","Max\n","\n","* 0: Cart Position $\\in [-4.8, 4.8]$\n","* 1: Cart Velocity $\\in [-\\infty, \\infty]$\n","* 2: Pole Angle $\\in [~ -0.418 rad (-24°), ~ 0.418 rad (24°)]$\n","* 3: Pole Angular Velocity $\\in [-\\infty,\\infty]$\n","\n","Note: While the ranges above denote the possible values for observation space of each element, it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n","\n","The cart x-position (index 0) can take values between (-4.8, 4.8), but the episode terminates if the cart leaves the (-2.4, 2.4) range.\n","\n","The pole angle can be observed between (-.418, .418) radians (or ±24°), but the episode terminates if the pole angle is not in the range (-.2095, .2095) (or ±12°)\n","\n","## Rewards\n","Since the goal is to keep the pole upright for as long as possible, a reward of +1 for every step taken, including the termination step, is allotted. The threshold for rewards is 475 for v1.\n","\n","## Starting State\n","All observations are assigned a uniformly random value in (-0.05, 0.05)\n","\n","## Episode Termination\n","The episode terminates if any one of the following occurs:\n","\n","* Pole Angle is greater than ±12°\n","* Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n","* Episode length is greater than 500 (200 for v0)"]},{"cell_type":"markdown","id":"f932d0ea","metadata":{"id":"f932d0ea"},"source":["1. We'll start by installing the gym libraries needed for simulating the environment"]},{"cell_type":"code","execution_count":null,"id":"0926a88d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22269,"status":"ok","timestamp":1707814327846,"user":{"displayName":"Sitou AFANOU","userId":"12894043844907935285"},"user_tz":-60},"id":"0926a88d","outputId":"b60e356d-dc2d-4671-88f6-dd73af7acbad"},"outputs":[],"source":["!pip install gym\n","!pip install gym[classic_control]"]},{"cell_type":"markdown","id":"ecb8dddd","metadata":{"id":"ecb8dddd"},"source":["2. We'll also proceed to the needed imports"]},{"cell_type":"code","execution_count":null,"id":"0fb4f53a","metadata":{"executionInfo":{"elapsed":953,"status":"ok","timestamp":1707814348538,"user":{"displayName":"Sitou AFANOU","userId":"12894043844907935285"},"user_tz":-60},"id":"0fb4f53a"},"outputs":[],"source":["import numpy as np # used for arrays\n","\n","import gym # pull the environment\n","\n","import time # to measure execution time\n","\n","import math # needed for calculations\n","\n","import matplotlib.pyplot as plt # for visualizing\n","\n","import pygame # has some effect on the rendering\n","\n","import matplotlib.animation as animation # for making gifs\n"]},{"cell_type":"markdown","id":"a73b3507","metadata":{"id":"a73b3507"},"source":["3. Let's create a variable `env` using the method described in the [documentation](https://www.gymlibrary.ml/environments/classic_control/cart_pole/)."]},{"cell_type":"code","execution_count":null,"id":"c14231ac","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":256,"status":"ok","timestamp":1707814350813,"user":{"displayName":"Sitou AFANOU","userId":"12894043844907935285"},"user_tz":-60},"id":"c14231ac","outputId":"5727a51e-da96-46a4-e52e-be44c13e4dfc"},"outputs":[],"source":["env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"]},{"cell_type":"markdown","id":"8b5600e7","metadata":{"id":"8b5600e7"},"source":["4. take a look at the action space."]},{"cell_type":"code","execution_count":null,"id":"52d8b5f3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":300,"status":"ok","timestamp":1707814357956,"user":{"displayName":"Sitou AFANOU","userId":"12894043844907935285"},"user_tz":-60},"id":"52d8b5f3","outputId":"60309449-dd86-4f56-836d-6c4db94c5124"},"outputs":[],"source":["env.action_space"]},{"cell_type":"markdown","id":"b713b4e8","metadata":{"id":"b713b4e8"},"source":["5. Take a look at the observation space"]},{"cell_type":"code","execution_count":null,"id":"5708d738","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":261,"status":"ok","timestamp":1707814371565,"user":{"displayName":"Sitou AFANOU","userId":"12894043844907935285"},"user_tz":-60},"id":"5708d738","outputId":"e88d9a56-b6f1-4710-a50b-08d661d38860"},"outputs":[],"source":["env.observation_space"]},{"cell_type":"markdown","id":"741089a0","metadata":{"id":"741089a0"},"source":["6. Reset the environment to take a look at a state"]},{"cell_type":"code","execution_count":null,"id":"a167e94b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a167e94b","outputId":"7c25549a-2207-431d-a605-5f1e439e6c65"},"outputs":[],"source":["env.reset()\n","# cart position , cart velocity, pole angle, pole angular velocity"]},{"cell_type":"markdown","id":"ce235b8c","metadata":{"id":"ce235b8c"},"source":["7. Since the environment is continuous in time, we'll make gifs to be able to visualize what the agent is doing. We'll take a few steps to do this:\n","    * Reset the environment\n","    * Define an empty list called `arr`\n","    * Set a variable `done` with value `False`\n","    * Set a variable `i` with value `0`\n","    * Start a while loop that will continue as long as `done` is `False` in the loop do:\n","        * append the visualization of the current state of the environment to the `arr` list (using `env.render(mode='rgb_array')` it should be a numpy array)\n","        * take a random action to produce the new observation\n","        * print the new observation"]},{"cell_type":"code","execution_count":null,"id":"3eb002ce","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3eb002ce","outputId":"e6527301-2ccb-4354-dbbd-e82c65d0902e"},"outputs":[],"source":["# returns an initial observation\n","env.reset()\n","\n","arr = []\n","done=False\n","i = 0\n","while done == False:\n","    # env.action_space.sample() produces either 0 (left) or 1 (right).\n","    arr.append(env.render()[0])\n","    observation, reward, done, info = env.step(env.action_space.sample())\n","    i+=1\n","    print(\"step\", i, observation, reward, done, info)"]},{"cell_type":"markdown","id":"27d6bdad","metadata":{"id":"27d6bdad"},"source":["8. Use the following command to make a gif out of the `arr` list of renderings"]},{"cell_type":"code","execution_count":null,"id":"29280b03","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29280b03","outputId":"3b073b40-733e-4747-965c-3d4bf3fd975a"},"outputs":[],"source":["!pip3 install pillow"]},{"cell_type":"code","execution_count":null,"id":"eaed8324","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"eaed8324","outputId":"09050c0f-c21e-4ddb-8483-f4ff48d28789"},"outputs":[],"source":["\"\"\"fig = plt.figure()\n","i=0\n","max_ = len(arr)\n","im = plt.imshow(arr[0], animated=True)\n","def updatefig(*args):\n","    global i\n","    global max_\n","    if (i<max_-1):\n","        i += 1\n","    else:\n","        i=0\n","    im.set_array(arr[i])\n","    return im,\n","ani = animation.FuncAnimation(fig, updatefig,  blit=True)\n","ani.save('cart_pole_random.gif', dpi=80, writer='pillow')\"\"\""]},{"cell_type":"code","execution_count":null,"id":"1e251d99","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1e251d99","outputId":"6a126757-b340-41bd-fe2f-6ecea2eaa43a"},"outputs":[],"source":["!pip3 install array2gif"]},{"cell_type":"code","execution_count":null,"id":"fca96cae","metadata":{"id":"fca96cae"},"outputs":[],"source":["from array2gif import write_gif\n","arr_rotated = [np.rot90(arri) for arri in arr]\n","write_gif(arr_rotated, 'cart_pole_random.gif', fps=6)"]},{"cell_type":"code","execution_count":null,"id":"IRbuJvJIb-Ar","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"IRbuJvJIb-Ar","outputId":"f4f91f63-99a2-4d06-a06b-eccf917e04e9"},"outputs":[],"source":["from IPython.display import Image\n","Image(open('cart_pole_random.gif','rb').read())"]},{"cell_type":"markdown","id":"3cefd81f","metadata":{"id":"3cefd81f"},"source":["9. Since the observation space is continuous and not discrete, we cannot straightforwardly apply Q-learning. The trick here is to convert this continuous state reinforcement learning problem into a discrete reinforcement learning problem by splitting the range of the different observation metrics into categories. Let's do that.\n","* What's the legal range of the four different metrics that define a non terminal state?\n","* For making the discretization function though, do no hesitate to assume larger ranges to avoid any errors"]},{"cell_type":"markdown","id":"1a64915b","metadata":{"id":"1a64915b"},"source":["* 0: Cart Position $\\in [-4.8, 4.8]$ but valid values are $\\in [-2.4, 2.4]$ round it to $[-3, 3]$\n","* 1: Cart Velocity $\\in [-\\infty, \\infty]$ but based on the observations they should be $\\in [-5,5]$\n","* 2: Pole Angle $\\in [~ -0.418 rad (-24°), ~ 0.418 rad (24°)]$ but valid values are $\\in [-.2095, .2095]$ let's round it to $[-.3, .3]$\n","* 3: Pole Angular Velocity $\\in [-\\infty,\\infty]$ but based on the observations they should be $\\in [-5,5]$"]},{"cell_type":"markdown","id":"6bf0266a","metadata":{"id":"6bf0266a"},"source":["10. We would like to split all of our state variables into 51 categories, set up an `Observation` object that is a list `[51,51,51,51]`"]},{"cell_type":"code","execution_count":null,"id":"b05a9c4b","metadata":{"id":"b05a9c4b"},"outputs":[],"source":["Observation = [51, 51, 51, 51]"]},{"cell_type":"markdown","id":"9055d013","metadata":{"id":"9055d013"},"source":["11. We need to define a function (we'll call it `get_discrete_state`) that turns a continuous state observation into a discrete state observation. The idea is to have 50 categories with category 0 corresponding to the lowest value, and 50 being the highest value. The input from the function should be a state, and the output should be a tuple of integers between 0 and 50."]},{"cell_type":"code","execution_count":null,"id":"a8176404","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a8176404","outputId":"2d69c7cc-05cf-4f4c-93dd-4f244dfa4adc"},"outputs":[],"source":["# setup a random state, a minimum legal state and maximum legal state\n","state = env.reset()[0]\n","state_min = np.array([-3,-5,-0.3,-5])\n","state_max = np.array([3,5,0.3,5])\n","state= [-2.9, -4.9, -0.29, -4.9]\n","def get_discrete_state(state):\n","    var_range = (state_max - state_min)/50\n","    discrete_state = state / var_range + [25,25,25,25]\n","    return tuple(discrete_state.astype(int))\n","print(state)\n","print(get_discrete_state(state))\n","print(get_discrete_state(state_min))\n","print(get_discrete_state(state_max))"]},{"cell_type":"markdown","id":"3af5c167","metadata":{"id":"3af5c167"},"source":["12. Create the Q table for the discretized reinforcement learning problem, initialize it with zeros. What is its shape?"]},{"cell_type":"code","execution_count":null,"id":"8c6f6731","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8c6f6731","outputId":"a043a63d-5459-4367-f8db-27a10f3cdbc1","scrolled":false},"outputs":[],"source":["q_table = np.zeros(shape=(Observation + [env.action_space.n]))\n","print(q_table.shape)\n","type(q_table)"]},{"cell_type":"markdown","id":"dc0ca8ea","metadata":{"id":"dc0ca8ea"},"source":["13. Let's now initialize the values we will need for the Q-learning algorithm to work:\n","* `LEARNING_RATE` = 0.1\n","* `DISCOUNT` = 0.95\n","* `EPISODES` = 60000\n","* `total` = 0\n","* `total_reward` = 0\n","* `prior_reward` = 0\n","* `epsilon` = 0.1"]},{"cell_type":"code","execution_count":null,"id":"f9137962","metadata":{"id":"f9137962"},"outputs":[],"source":["LEARNING_RATE = 0.1\n","DISCOUNT = 0.95\n","EPISODES = 60000\n","total = 0\n","total_reward = 0\n","prior_reward = 0\n","epsilon = 0.1"]},{"cell_type":"markdown","id":"a910952f","metadata":{"id":"a910952f"},"source":["14. Let's now code the Q-learning algorithm, here are the steps:\n","* Loop over the number of episodes\n","    * reset the environment\n","    * discretize the initial state\n","    * setup `done=False`\n","    * setup `episode_reward=0`\n","    * loop until `done` is `True`\n","        * setup conditions to implement the $\\epsilon-greedy$ policy\n","        * take a step with the chosen action\n","        * increment the `episode_reward`\n","        * discretize the new state\n","        * if the state is not terminal:\n","            * update the Q-table the Q-learning update rule\n","        * replace current discrete state with new discrete state\n","\n","As sanity check, print the average `episode_reward` calculated over the last 1000 episodes."]},{"cell_type":"code","execution_count":null,"id":"f14586de","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"f14586de","outputId":"9c187c86-283a-4998-9b8c-55eebff098fb"},"outputs":[],"source":["episode_reward_list = [] # to calculate average reward across the episodes\n","\n","# strting the loop\n","for episode in range(EPISODES+1):\n","    discrete_state = get_discrete_state(env.reset()[0]) # discretize the initial state\n","    done = False # initialize done\n","    episode_reward = 0 # initialize episode reward\n","\n","    while not done: # loop until termination of an episode\n","        if np.random.random() < epsilon: # random action with probability epsilon\n","            action = env.action_space.sample()\n","        elif np.max(q_table[discrete_state]) == np.min(q_table[discrete_state]): # if no best action can be found, pick a random action\n","            action = env.action_space.sample()\n","        else: # pick greedy action with probability 1-epsilon\n","            action = np.argmax(q_table[discrete_state])\n","\n","        new_state, reward, done, _ = env.step(action) #step action to get new states, reward, and the \"done\" status.\n","\n","        episode_reward += reward #add the reward\n","\n","        new_discrete_state = get_discrete_state(new_state) # discretize new state\n","\n","        if not done: #update q-table\n","            max_future_q = np.max(q_table[new_discrete_state]) # value of the next best action\n","\n","            current_q = q_table[discrete_state + (action,)] # value of the current action\n","\n","            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q) # q learning update\n","\n","            q_table[discrete_state + (action,)] = new_q # set new value in q table\n","\n","        discrete_state = new_discrete_state # replace current state with new state\n","\n","    episode_reward_list.append(episode_reward) # add the episode reward to the list\n","\n","    if episode % 1000 == 0: # calculate the average reward across the last 1000 episodes and reinitialize the reward list\n","        print(\"For episode:\", episode, \"the reward was:\", np.mean(episode_reward_list))\n","        episode_reward_list = []"]},{"cell_type":"markdown","id":"4290c522","metadata":{"id":"4290c522"},"source":["15. Now that the training is done, it seems as though things turned out really well! Reuse the code to make the animation in order to display the behaviour of the trained agent on the CartPole game."]},{"cell_type":"code","execution_count":null,"id":"6f69836f","metadata":{"id":"6f69836f"},"outputs":[],"source":["# returns an initial observation\n","observation = env.reset()[0]\n","\n","arr = []\n","done=False\n","i = 0\n","while done != True:\n","    arr.append(env.render())\n","    discrete_state = get_discrete_state(observation)\n","    action = np.argmax(q_table[discrete_state]) #take greedy action\n","    observation, reward, done, info,_ = env.step(action)\n","    i+=1"]},{"cell_type":"code","execution_count":null,"id":"b1c6b543","metadata":{"id":"b1c6b543","outputId":"1470a03d-8a84-4642-975f-0f8c67a53a5e"},"outputs":[],"source":["\"\"\"fig = plt.figure()\n","i=0\n","max_ = len(arr)\n","im = plt.imshow(arr[0], animated=True)\n","def updatefig(*args):\n","    global i\n","    global max_\n","    if (i<max_-1):\n","        i += 1\n","    else:\n","        i=0\n","    im.set_array(arr[i])\n","    return im,\n","ani = animation.FuncAnimation(fig, updatefig,  blit=True)\n","ani.save('cart_pole_trained.gif', dpi=80, writer='imagemagick')\"\"\""]},{"cell_type":"code","execution_count":null,"id":"528b4d0c","metadata":{"id":"528b4d0c"},"outputs":[],"source":["from array2gif import write_gif\n","arr_rotated = [np.rot90(arri) for arri in arr]\n","write_gif(arr_rotated, 'cart_pole_trained.gif', fps=6)"]},{"cell_type":"markdown","id":"09d2117f","metadata":{"id":"09d2117f"},"source":["<img src=\"cart_pole_trained.gif\" width=\"750\" align=\"center\">"]},{"cell_type":"markdown","id":"a5ed9e99","metadata":{"id":"a5ed9e99"},"source":["Congratulations! You have successfully solved a reinforcement learning problem with a continuous state space! The discretization technique is very common when faced with a fairly simple continuous state space like this one! Another approach would be to rely on function approximation in order to map the states and actions to different values in order to pick actions using a function (and this is where Deep Neural Networks may play a significant role!).\n","\n","NB: Note that discretizing a problem like this to use classic Q-learning is in itself already a function approximation ;)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"},"vscode":{"interpreter":{"hash":"fbc4d3870518eee81184ced0d2279c769a0eca59aab465c4e7ec13e5e6c47a3e"}}},"nbformat":4,"nbformat_minor":5}
